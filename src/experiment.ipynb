{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 20:06:51.235 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "os.environ['PYTHONPATH'] = os.getcwd()\n",
    "os.environ['GITHUB_AUTH_TOKEN'] = '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'\n",
    "from backend.evaluator.paper.paper import Paper\n",
    "from backend.evaluator.repository.github.github import Github\n",
    "from backend.tools.depot import Depot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you've either downloaded from depot/ folder from the drive or have the below structure setup \n",
    "# - depot/\n",
    "#   - papers/\n",
    "#     - authors/\n",
    "#   - repository/\n",
    "#     - owner/\n",
    "#     - organization/ \n",
    "#     - member/\n",
    "\n",
    "depot = Depot(\n",
    "    root_path='../depot', \n",
    "    paper_path='../depot/papers',\n",
    "    repo_path='../depot/repository'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git = Github(owner='Vishal-S-P', repo='vishal-s-p.github.io', branch='master')\n",
    "paper = Paper('https://arxiv.org/abs/2107.06278')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 20:06:51 - Metadata - INFO - Metadata object created\n",
      "2024-02-23 20:06:51 - Metadata - INFO - Generating root metadata...\n",
      "2024-02-23 20:06:51 - Metadata - INFO - Processing root metadata...\n",
      "2024-02-23 20:06:51.264 WARNING streamlit.runtime.state.session_state_proxy: Session state does not function when running a script without `streamlit run`\n",
      "2024-02-23 20:06:51 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io/contributors?state=all&page=1&per_page=100\n",
      "2024-02-23 20:06:51 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io/contributors?state=all&page=2&per_page=100\n",
      "2024-02-23 20:06:52 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io/commits?state=all&page=1&per_page=100\n",
      "2024-02-23 20:06:52 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io/commits?state=all&page=2&per_page=100\n",
      "2024-02-23 20:06:53 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io/commits?state=all&page=3&per_page=100\n",
      "2024-02-23 20:06:53 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io\n",
      "2024-02-23 20:06:54 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:06:54 - Metadata - INFO - Root metadata generated.\n",
      "2024-02-23 20:06:54 - Metadata - INFO - Generating organization metadata...\n",
      "2024-02-23 20:06:54 - Metadata - INFO - Repo not affiliated with Organization...\n",
      "2024-02-23 20:06:54 - Metadata - INFO - Organization metadata generated.\n",
      "2024-02-23 20:06:54 - Metadata - INFO - Generating owner metadata...\n",
      "2024-02-23 20:06:54 - Metadata - INFO - Processing owner metadata...\n",
      "2024-02-23 20:06:54 - Metadata - INFO - Fetching https://api.github.com/users/Vishal-S-P/followers?state=all&page=1&per_page=100\n",
      "2024-02-23 20:06:55 - Metadata - INFO - Fetching https://api.github.com/users/Vishal-S-P/followers?state=all&page=2&per_page=100\n",
      "2024-02-23 20:06:56 - Metadata - INFO - Fetching https://api.github.com/users/Vishal-S-P/repos?state=all&page=1&per_page=100\n",
      "2024-02-23 20:06:57 - Metadata - INFO - Fetching https://api.github.com/users/Vishal-S-P/repos?state=all&page=2&per_page=100\n",
      "2024-02-23 20:06:57 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/AdvancedOptML\n",
      "2024-02-23 20:06:58 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/AdvancedOptML/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:06:58 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-active-learning\n",
      "2024-02-23 20:06:59 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-active-learning/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:06:59 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Deblurring\n",
      "2024-02-23 20:07:00 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Deblurring/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:00 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-information-geometry\n",
      "2024-02-23 20:07:01 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-information-geometry/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:01 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-multimodal-ml\n",
      "2024-02-23 20:07:02 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-multimodal-ml/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:02 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Multimodal-Research\n",
      "2024-02-23 20:07:03 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Multimodal-Research/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:03 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-NeRF\n",
      "2024-02-23 20:07:04 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-NeRF/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:04 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Vision-Attentions\n",
      "2024-02-23 20:07:05 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Vision-Attentions/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:05 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/clean-fid\n",
      "2024-02-23 20:07:06 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/clean-fid/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:06 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/DataFree\n",
      "2024-02-23 20:07:07 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/DataFree/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:07 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/DDN\n",
      "2024-02-23 20:07:07 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/DDN/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:08 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/EI\n",
      "2024-02-23 20:07:08 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/EI/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:09 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/FastGAN-pytorch\n",
      "2024-02-23 20:07:09 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/FastGAN-pytorch/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:10 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/google-research\n",
      "2024-02-23 20:07:10 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/google-research/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:11 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/IF\n",
      "2024-02-23 20:07:11 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/IF/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:12 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Image_Processing\n",
      "2024-02-23 20:07:12 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Image_Processing/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:13 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/latent-diffusion\n",
      "2024-02-23 20:07:18 - Metadata - ERROR - Timeout occurred while fetching https://api.github.com/repos/Vishal-S-P/latent-diffusion. Retrying in 15 seconds...\n",
      "2024-02-23 20:07:34 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/latent-diffusion/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:34 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Learning-to-Extract-a-Video-Sequence-from-a-Single-Motion-Blurred-Image\n",
      "2024-02-23 20:07:35 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Learning-to-Extract-a-Video-Sequence-from-a-Single-Motion-Blurred-Image/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:35 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Llama-X\n",
      "2024-02-23 20:07:36 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Llama-X/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:36 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/LMOps\n",
      "2024-02-23 20:07:37 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/LMOps/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:37 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/ML-Fundamentals-Reading-Lists\n",
      "2024-02-23 20:07:37 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/ML-Fundamentals-Reading-Lists/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:38 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Neural-Network-Architecture-Diagrams\n",
      "2024-02-23 20:07:38 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Neural-Network-Architecture-Diagrams/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:39 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/PGD-Net\n",
      "2024-02-23 20:07:39 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/PGD-Net/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:40 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Poisoning-Instruction-Tuned-Models\n",
      "2024-02-23 20:07:41 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Poisoning-Instruction-Tuned-Models/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:41 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/REI\n",
      "2024-02-23 20:07:41 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/REI/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:42 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Reproducible-Deep-Compressive-Sensing\n",
      "2024-02-23 20:07:42 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Reproducible-Deep-Compressive-Sensing/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:43 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/SciencePlots\n",
      "2024-02-23 20:07:43 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/SciencePlots/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:44 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/sgilo\n",
      "2024-02-23 20:07:44 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/sgilo/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:45 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/sngan_projection\n",
      "2024-02-23 20:07:45 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/sngan_projection/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:46 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/stable-diffusion-webui\n",
      "2024-02-23 20:07:46 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/stable-diffusion-webui/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:47 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/torch-fidelity\n",
      "2024-02-23 20:07:47 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/torch-fidelity/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:48 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/tpu-starter\n",
      "2024-02-23 20:07:48 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/tpu-starter/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:49 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Unsupervised-Learning-From-Incomplete-Measurements-for-Inverse-Problems\n",
      "2024-02-23 20:07:49 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Unsupervised-Learning-From-Incomplete-Measurements-for-Inverse-Problems/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:50 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io\n",
      "2024-02-23 20:07:50 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:07:51 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/website\n",
      "2024-02-23 20:07:56 - Metadata - ERROR - Timeout occurred while fetching https://api.github.com/repos/Vishal-S-P/website. Retrying in 15 seconds...\n",
      "2024-02-23 20:08:11 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/website/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:12 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/yolo_official_model\n",
      "2024-02-23 20:08:12 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/yolo_official_model/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:13 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/ziplora-pytorch\n",
      "2024-02-23 20:08:13 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/ziplora-pytorch/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:14 - Metadata - INFO - Owner metadata generated.\n",
      "2024-02-23 20:08:14 - Metadata - INFO - Generating member metadata...\n",
      "2024-02-23 20:08:14 - Metadata - INFO - Processing member metadata...\n",
      "2024-02-23 20:08:14 - Metadata - INFO - Fetching https://api.github.com/users/Vishal-S-P/followers?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:14 - Metadata - INFO - Fetching https://api.github.com/users/Vishal-S-P/followers?state=all&page=2&per_page=100\n",
      "2024-02-23 20:08:15 - Metadata - INFO - Fetching https://api.github.com/users/Vishal-S-P/repos?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:15 - Metadata - INFO - Fetching https://api.github.com/users/Vishal-S-P/repos?state=all&page=2&per_page=100\n",
      "2024-02-23 20:08:16 - Metadata - INFO - Processed metadata for member: Vishal-S-P\n",
      "2024-02-23 20:08:16 - Metadata - INFO - Fetching https://api.github.com/users/JyotiP10/followers?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:16 - Metadata - INFO - Fetching https://api.github.com/users/JyotiP10/repos?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:17 - Metadata - INFO - Processed metadata for member: JyotiP10\n",
      "2024-02-23 20:08:17 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/AdvancedOptML\n",
      "2024-02-23 20:08:17 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/AdvancedOptML/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:18 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-active-learning\n",
      "2024-02-23 20:08:18 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-active-learning/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:19 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Deblurring\n",
      "2024-02-23 20:08:20 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Deblurring/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:21 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-information-geometry\n",
      "2024-02-23 20:08:26 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-information-geometry/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:27 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-multimodal-ml\n",
      "2024-02-23 20:08:27 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-multimodal-ml/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:28 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Multimodal-Research\n",
      "2024-02-23 20:08:28 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Multimodal-Research/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:29 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-NeRF\n",
      "2024-02-23 20:08:29 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/awesome-NeRF/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:30 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Vision-Attentions\n",
      "2024-02-23 20:08:30 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Awesome-Vision-Attentions/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:31 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/clean-fid\n",
      "2024-02-23 20:08:31 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/clean-fid/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:31 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/DataFree\n",
      "2024-02-23 20:08:32 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/DataFree/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:32 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/DDN\n",
      "2024-02-23 20:08:33 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/DDN/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:33 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/EI\n",
      "2024-02-23 20:08:34 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/EI/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:34 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/FastGAN-pytorch\n",
      "2024-02-23 20:08:35 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/FastGAN-pytorch/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:35 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/google-research\n",
      "2024-02-23 20:08:36 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/google-research/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:36 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/IF\n",
      "2024-02-23 20:08:37 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/IF/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:37 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Image_Processing\n",
      "2024-02-23 20:08:38 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Image_Processing/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:38 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/latent-diffusion\n",
      "2024-02-23 20:08:39 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/latent-diffusion/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:39 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Learning-to-Extract-a-Video-Sequence-from-a-Single-Motion-Blurred-Image\n",
      "2024-02-23 20:08:40 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Learning-to-Extract-a-Video-Sequence-from-a-Single-Motion-Blurred-Image/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:40 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Llama-X\n",
      "2024-02-23 20:08:41 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Llama-X/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:41 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/LMOps\n",
      "2024-02-23 20:08:42 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/LMOps/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:42 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/ML-Fundamentals-Reading-Lists\n",
      "2024-02-23 20:08:43 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/ML-Fundamentals-Reading-Lists/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:43 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Neural-Network-Architecture-Diagrams\n",
      "2024-02-23 20:08:44 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Neural-Network-Architecture-Diagrams/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:44 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/PGD-Net\n",
      "2024-02-23 20:08:44 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/PGD-Net/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:45 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Poisoning-Instruction-Tuned-Models\n",
      "2024-02-23 20:08:45 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Poisoning-Instruction-Tuned-Models/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:46 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/REI\n",
      "2024-02-23 20:08:46 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/REI/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:47 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Reproducible-Deep-Compressive-Sensing\n",
      "2024-02-23 20:08:47 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Reproducible-Deep-Compressive-Sensing/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:48 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/SciencePlots\n",
      "2024-02-23 20:08:48 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/SciencePlots/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:49 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/sgilo\n",
      "2024-02-23 20:08:49 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/sgilo/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:50 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/sngan_projection\n",
      "2024-02-23 20:08:50 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/sngan_projection/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:51 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/stable-diffusion-webui\n",
      "2024-02-23 20:08:51 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/stable-diffusion-webui/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:52 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/torch-fidelity\n",
      "2024-02-23 20:08:52 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/torch-fidelity/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:53 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/tpu-starter\n",
      "2024-02-23 20:08:53 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/tpu-starter/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:54 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Unsupervised-Learning-From-Incomplete-Measurements-for-Inverse-Problems\n",
      "2024-02-23 20:08:54 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/Unsupervised-Learning-From-Incomplete-Measurements-for-Inverse-Problems/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:54 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io\n",
      "2024-02-23 20:08:55 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:55 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/website\n",
      "2024-02-23 20:08:56 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/website/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:56 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/yolo_official_model\n",
      "2024-02-23 20:08:57 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/yolo_official_model/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:57 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/ziplora-pytorch\n",
      "2024-02-23 20:08:58 - Metadata - INFO - Fetching https://api.github.com/repos/Vishal-S-P/ziplora-pytorch/pulls?state=all&page=1&per_page=100\n",
      "2024-02-23 20:08:58 - Metadata - INFO - Processed user repo data for member: Vishal-S-P\n",
      "2024-02-23 20:08:58 - Metadata - ERROR - Error processing user repo data for member JyotiP10: local variable 'repo' referenced before assignment\n",
      "2024-02-23 20:08:58 - Metadata - INFO - Member metadata generated.\n"
     ]
    }
   ],
   "source": [
    "git.get_meta() # Gets meta from a mapping in metadata.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 20:08:58 - Tree - INFO - Initializing Tree object...\n",
      "2024-02-23 20:08:58 - Tree - INFO - Starting to make tree with provided repo data... https://api.github.com/repos/Vishal-S-P/vishal-s-p.github.io/git/trees/master?recursive=0\n",
      "2024-02-23 20:08:58.970 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/aksha/.pyenv/versions/3.9.18/lib/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-02-23 20:08:58.971 No runtime found, using MemoryCacheStorageManager\n",
      "2024-02-23 20:08:59 - Tree - INFO - Tree successfully created.\n",
      "2024-02-23 20:08:59 - Tree - INFO - Tree object initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---| .all-contributorsrc\n",
      "---| .dockerignore\n",
      "---| .github/\n",
      "------| .github/ISSUE_TEMPLATE/\n",
      "---------| .github/ISSUE_TEMPLATE/bug_report.md\n",
      "---------| .github/ISSUE_TEMPLATE/feature_request.md\n",
      "------| .github/stale.yml\n",
      "------| .github/workflows/\n",
      "---------| .github/workflows/deploy-docker-tag.yml\n",
      "---------| .github/workflows/deploy-image.yml\n",
      "---------| .github/workflows/deploy.yml\n",
      "---| .gitignore\n",
      "---| .pre-commit-config.yaml\n",
      "---| CONTRIBUTING.md\n",
      "---| Gemfile\n",
      "---| LICENSE\n",
      "---| README.md\n",
      "---| assets/\n",
      "------| assets/bibliography/\n",
      "---------| assets/bibliography/2018-12-22-distill.bib\n",
      "------| assets/css/\n",
      "---------| assets/css/github.css\n",
      "---------| assets/css/main.css\n",
      "---------| assets/css/style.css\n",
      "------| assets/img/\n",
      "---------| assets/img/ece_logo.png\n",
      "---------| assets/img/gallery/\n",
      "------------| assets/img/gallery/img1.jpeg\n",
      "------------| assets/img/gallery/img10.jpeg\n",
      "------------| assets/img/gallery/img11.jpeg\n",
      "------------| assets/img/gallery/img12.jpeg\n",
      "------------| assets/img/gallery/img13.jpeg\n",
      "------------| assets/img/gallery/img14.jpeg\n",
      "------------| assets/img/gallery/img15.jpeg\n",
      "------------| assets/img/gallery/img16.jpeg\n",
      "------------| assets/img/gallery/img17.jpeg\n",
      "------------| assets/img/gallery/img18.jpeg\n",
      "------------| assets/img/gallery/img19.jpeg\n",
      "------------| assets/img/gallery/img2.jpeg\n",
      "------------| assets/img/gallery/img20.jpeg\n",
      "------------| assets/img/gallery/img21.jpeg\n",
      "------------| assets/img/gallery/img22.jpeg\n",
      "------------| assets/img/gallery/img23.png\n",
      "------------| assets/img/gallery/img24.png\n",
      "------------| assets/img/gallery/img25.png\n",
      "------------| assets/img/gallery/img26.png\n",
      "------------| assets/img/gallery/img27.png\n",
      "------------| assets/img/gallery/img3.jpeg\n",
      "------------| assets/img/gallery/img4.jpeg\n",
      "------------| assets/img/gallery/img5.jpeg\n",
      "------------| assets/img/gallery/img6.jpeg\n",
      "------------| assets/img/gallery/img7.jpeg\n",
      "------------| assets/img/gallery/img8.jpeg\n",
      "------------| assets/img/gallery/img9.jpeg\n",
      "---------| assets/img/profile_pic.jpg\n",
      "---------| assets/img/projects/\n",
      "------------| assets/img/projects/detectors/\n",
      "---------------| assets/img/projects/detectors/cover.png\n",
      "---------------| assets/img/projects/detectors/minicover.png\n",
      "------| assets/js/\n",
      "---------| assets/js/common.js\n",
      "---------| assets/js/dark_mode.js\n",
      "---------| assets/js/distillpub/\n",
      "------------| assets/js/distillpub/template.v2.js\n",
      "------------| assets/js/distillpub/template.v2.js.map\n",
      "------------| assets/js/distillpub/transforms.v2.js\n",
      "------------| assets/js/distillpub/transforms.v2.js.map\n",
      "---------| assets/js/mansory.js\n",
      "---------| assets/js/theme.js\n",
      "------| assets/pdf/\n",
      "---------| assets/pdf/resume.pdf\n",
      "---| bin/\n",
      "------| bin/cibuild\n",
      "------| bin/deploy\n",
      "---| blog/\n",
      "------| blog/2015/\n",
      "---------| blog/2015/code/\n",
      "------------| blog/2015/code/index.html\n",
      "---------| blog/2015/comments/\n",
      "------------| blog/2015/comments/index.html\n",
      "---------| blog/2015/formatting-and-links/\n",
      "------------| blog/2015/formatting-and-links/index.html\n",
      "---------| blog/2015/images/\n",
      "------------| blog/2015/images/index.html\n",
      "---------| blog/2015/math/\n",
      "------------| blog/2015/math/index.html\n",
      "------| blog/2018/\n",
      "---------| blog/2018/distill/\n",
      "------------| blog/2018/distill/index.html\n",
      "------| blog/2020/\n",
      "---------| blog/2020/github-metadata/\n",
      "------------| blog/2020/github-metadata/index.html\n",
      "---------| blog/2020/twitter/\n",
      "------------| blog/2020/twitter/index.html\n",
      "------| blog/index.html\n",
      "------| blog/page/\n",
      "---------| blog/page/2/\n",
      "------------| blog/page/2/index.html\n",
      "---------| blog/page/3/\n",
      "------------| blog/page/3/index.html\n",
      "---| cv/\n",
      "------| cv/index.html\n",
      "---| index.html\n",
      "---| moments/\n",
      "------| moments/css/\n",
      "---------| moments/css/custom.css\n",
      "---------| moments/css/style.blue.css\n",
      "---------| moments/css/style.default.css\n",
      "---------| moments/css/style.green.css\n",
      "---------| moments/css/style.pink.css\n",
      "---------| moments/css/style.red.css\n",
      "---------| moments/css/style.sea.css\n",
      "---------| moments/css/style.violet.css\n",
      "------| moments/img/\n",
      "---------| moments/img/portfolio/\n",
      "------------| moments/img/portfolio/IMG_1.png\n",
      "------------| moments/img/portfolio/IMG_2.png\n",
      "------------| moments/img/portfolio/IMG_3.png\n",
      "------------| moments/img/portfolio/IMG_4.png\n",
      "------| moments/index.html\n",
      "------| moments/js/\n",
      "---------| moments/js/front.js\n",
      "---------| moments/js/fslightbox.js\n",
      "---| news/\n",
      "------| news/announcement_1/\n",
      "---------| news/announcement_1/index.html\n",
      "------| news/announcement_2/\n",
      "---------| news/announcement_2/index.html\n",
      "------| news/announcement_3/\n",
      "---------| news/announcement_3/index.html\n",
      "------| news/announcement_4/\n",
      "---------| news/announcement_4/index.html\n",
      "---| projects/\n",
      "------| projects/2023/\n",
      "---------| projects/2023/proj_1.html\n",
      "---------| projects/2023/proj_2.html\n",
      "---------| projects/2023/static/\n",
      "------------| projects/2023/static/css/\n",
      "---------------| projects/2023/static/css/bulma-carousel.min.css\n",
      "---------------| projects/2023/static/css/bulma-slider.min.css\n",
      "---------------| projects/2023/static/css/bulma.css.map.txt\n",
      "---------------| projects/2023/static/css/bulma.min.css\n",
      "---------------| projects/2023/static/css/fontawesome.all.min.css\n",
      "---------------| projects/2023/static/css/index.css\n",
      "------------| projects/2023/static/images/\n",
      "---------------| projects/2023/static/images/github_icon.jpg\n",
      "---------------| projects/2023/static/images/sadtalker.jpg\n",
      "---------------| projects/2023/static/images/slide_icon.jpg\n",
      "---------------| projects/2023/static/images/youtube_icon.jpg\n",
      "------------| projects/2023/static/js/\n",
      "---------------| projects/2023/static/js/bulma-carousel.js\n",
      "---------------| projects/2023/static/js/bulma-carousel.min.js\n",
      "---------------| projects/2023/static/js/bulma-slider.js\n",
      "---------------| projects/2023/static/js/bulma-slider.min.js\n",
      "---------------| projects/2023/static/js/fontawesome.all.min.js\n",
      "---------------| projects/2023/static/js/index.js\n",
      "------------| projects/2023/static/videos/\n",
      "---------------| projects/2023/static/videos/ablation_ExpNet.mp4\n",
      "---------------| projects/2023/static/videos/ablation_facerender.mp4\n",
      "---------------| projects/2023/static/videos/chinese_speaker.mp4\n",
      "---------------| projects/2023/static/videos/compare.mp4\n",
      "---------------| projects/2023/static/videos/comparison_crossid.mp4\n",
      "---------------| projects/2023/static/videos/comparisonmp4.mp4\n",
      "---------------| projects/2023/static/videos/different_audio_same_style.mp4\n",
      "---------------| projects/2023/static/videos/different_style_same_audio.mp4\n",
      "---------------| projects/2023/static/videos/eye blinking.mp4\n",
      "---------------| projects/2023/static/videos/sadtalker_supp.mp4\n",
      "---------------| projects/2023/static/videos/sing.mp4\n",
      "---------------| projects/2023/static/videos/talking.mp4\n",
      "------| projects/index.html\n",
      "------| projects/projects.js\n",
      "---| publications/\n",
      "------| publications/index.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "git.get_tree() # Basic nested tree \n",
    "print(str(git))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"path\": \"\",\n",
      "  \"children\": [\n",
      "    {\n",
      "      \"path\": \".all-contributorsrc\",\n",
      "      \"type\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \".dockerignore\",\n",
      "      \"type\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \".github\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"path\": \"ISSUE_TEMPLATE\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"bug_report.md\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"feature_request.md\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 0,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 2,\n",
      "          \"number_of_folders\": 0,\n",
      "          \"file_types\": [\n",
      "            \"md\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"stale.yml\",\n",
      "          \"type\": \"file\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"workflows\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"deploy-docker-tag.yml\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"deploy-image.yml\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"deploy.yml\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 0,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 3,\n",
      "          \"number_of_folders\": 0,\n",
      "          \"file_types\": [\n",
      "            \"yml\"\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"type\": \"folder\",\n",
      "      \"number_of_subfolders\": 2,\n",
      "      \"number_of_subfiles\": 0,\n",
      "      \"number_of_files\": 1,\n",
      "      \"number_of_folders\": 2,\n",
      "      \"file_types\": [\n",
      "        \"md\",\n",
      "        \"yml\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"path\": \".gitignore\",\n",
      "      \"type\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \".pre-commit-config.yaml\",\n",
      "      \"type\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"CONTRIBUTING.md\",\n",
      "      \"type\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"Gemfile\",\n",
      "      \"type\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"LICENSE\",\n",
      "      \"type\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"README.md\",\n",
      "      \"type\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"assets\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"path\": \"bibliography\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"2018-12-22-distill.bib\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 0,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 1,\n",
      "          \"number_of_folders\": 0,\n",
      "          \"file_types\": [\n",
      "            \"bib\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"css\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"github.css\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"main.css\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"style.css\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 0,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 3,\n",
      "          \"number_of_folders\": 0,\n",
      "          \"file_types\": [\n",
      "            \"css\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"img\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"ece_logo.png\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"gallery\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"img1.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img10.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img11.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img12.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img13.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img14.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img15.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img16.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img17.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img18.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img19.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img2.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img20.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img21.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img22.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img23.png\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img24.png\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img25.png\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img26.png\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img27.png\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img3.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img4.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img5.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img6.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img7.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img8.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"img9.jpeg\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 27,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"png\",\n",
      "                \"jpeg\"\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"profile_pic.jpg\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"projects\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"detectors\",\n",
      "                  \"children\": [\n",
      "                    {\n",
      "                      \"path\": \"cover.png\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"minicover.png\",\n",
      "                      \"type\": \"file\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"type\": \"folder\",\n",
      "                  \"number_of_subfolders\": 0,\n",
      "                  \"number_of_subfiles\": 0,\n",
      "                  \"number_of_files\": 2,\n",
      "                  \"number_of_folders\": 0,\n",
      "                  \"file_types\": [\n",
      "                    \"png\"\n",
      "                  ]\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 1,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 0,\n",
      "              \"number_of_folders\": 1,\n",
      "              \"file_types\": [\n",
      "                \"png\"\n",
      "              ]\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 3,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 2,\n",
      "          \"number_of_folders\": 2,\n",
      "          \"file_types\": [\n",
      "            \"png\",\n",
      "            \"jpg\",\n",
      "            \"jpeg\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"js\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"common.js\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"dark_mode.js\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"distillpub\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"template.v2.js\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"template.v2.js.map\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"transforms.v2.js\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"transforms.v2.js.map\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 4,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"js\",\n",
      "                \"map\"\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"mansory.js\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"theme.js\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 1,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 4,\n",
      "          \"number_of_folders\": 1,\n",
      "          \"file_types\": [\n",
      "            \"js\",\n",
      "            \"map\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"pdf\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"resume.pdf\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 0,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 1,\n",
      "          \"number_of_folders\": 0,\n",
      "          \"file_types\": [\n",
      "            \"pdf\"\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"type\": \"folder\",\n",
      "      \"number_of_subfolders\": 9,\n",
      "      \"number_of_subfiles\": 0,\n",
      "      \"number_of_files\": 0,\n",
      "      \"number_of_folders\": 5,\n",
      "      \"file_types\": [\n",
      "        \"jpg\",\n",
      "        \"js\",\n",
      "        \"css\",\n",
      "        \"jpeg\",\n",
      "        \"map\",\n",
      "        \"png\",\n",
      "        \"pdf\",\n",
      "        \"bib\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"bin\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"path\": \"cibuild\",\n",
      "          \"type\": \"file\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"deploy\",\n",
      "          \"type\": \"file\"\n",
      "        }\n",
      "      ],\n",
      "      \"type\": \"folder\",\n",
      "      \"number_of_subfolders\": 0,\n",
      "      \"number_of_subfiles\": 0,\n",
      "      \"number_of_files\": 2,\n",
      "      \"number_of_folders\": 0,\n",
      "      \"file_types\": [\n",
      "        \"cibuild\",\n",
      "        \"deploy\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"blog\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"path\": \"2015\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"code\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"index.html\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 1,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"html\"\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"comments\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"index.html\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 1,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"html\"\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"formatting-and-links\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"index.html\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 1,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"html\"\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"images\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"index.html\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 1,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"html\"\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"math\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"index.html\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 1,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"html\"\n",
      "              ]\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 5,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 0,\n",
      "          \"number_of_folders\": 5,\n",
      "          \"file_types\": [\n",
      "            \"html\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"2018\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"distill\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"index.html\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 1,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"html\"\n",
      "              ]\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 1,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 0,\n",
      "          \"number_of_folders\": 1,\n",
      "          \"file_types\": [\n",
      "            \"html\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"2020\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"github-metadata\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"index.html\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 1,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"html\"\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"twitter\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"index.html\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 1,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"html\"\n",
      "              ]\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 2,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 0,\n",
      "          \"number_of_folders\": 2,\n",
      "          \"file_types\": [\n",
      "            \"html\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"index.html\",\n",
      "          \"type\": \"file\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"page\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"2\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"index.html\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 1,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"html\"\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"3\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"index.html\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 1,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"html\"\n",
      "              ]\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 2,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 0,\n",
      "          \"number_of_folders\": 2,\n",
      "          \"file_types\": [\n",
      "            \"html\"\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"type\": \"folder\",\n",
      "      \"number_of_subfolders\": 14,\n",
      "      \"number_of_subfiles\": 0,\n",
      "      \"number_of_files\": 1,\n",
      "      \"number_of_folders\": 4,\n",
      "      \"file_types\": [\n",
      "        \"html\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"cv\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"path\": \"index.html\",\n",
      "          \"type\": \"file\"\n",
      "        }\n",
      "      ],\n",
      "      \"type\": \"folder\",\n",
      "      \"number_of_subfolders\": 0,\n",
      "      \"number_of_subfiles\": 0,\n",
      "      \"number_of_files\": 1,\n",
      "      \"number_of_folders\": 0,\n",
      "      \"file_types\": [\n",
      "        \"html\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"index.html\",\n",
      "      \"type\": \"file\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"moments\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"path\": \"css\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"custom.css\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"style.blue.css\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"style.default.css\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"style.green.css\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"style.pink.css\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"style.red.css\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"style.sea.css\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"style.violet.css\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 0,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 8,\n",
      "          \"number_of_folders\": 0,\n",
      "          \"file_types\": [\n",
      "            \"css\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"img\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"portfolio\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"IMG_1.png\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"IMG_2.png\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"IMG_3.png\",\n",
      "                  \"type\": \"file\"\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"IMG_4.png\",\n",
      "                  \"type\": \"file\"\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 0,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 4,\n",
      "              \"number_of_folders\": 0,\n",
      "              \"file_types\": [\n",
      "                \"png\"\n",
      "              ]\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 1,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 0,\n",
      "          \"number_of_folders\": 1,\n",
      "          \"file_types\": [\n",
      "            \"png\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"index.html\",\n",
      "          \"type\": \"file\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"js\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"front.js\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"fslightbox.js\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 0,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 2,\n",
      "          \"number_of_folders\": 0,\n",
      "          \"file_types\": [\n",
      "            \"js\"\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"type\": \"folder\",\n",
      "      \"number_of_subfolders\": 4,\n",
      "      \"number_of_subfiles\": 0,\n",
      "      \"number_of_files\": 1,\n",
      "      \"number_of_folders\": 3,\n",
      "      \"file_types\": [\n",
      "        \"png\",\n",
      "        \"html\",\n",
      "        \"css\",\n",
      "        \"js\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"news\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"path\": \"announcement_1\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"index.html\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 0,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 1,\n",
      "          \"number_of_folders\": 0,\n",
      "          \"file_types\": [\n",
      "            \"html\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"announcement_2\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"index.html\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 0,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 1,\n",
      "          \"number_of_folders\": 0,\n",
      "          \"file_types\": [\n",
      "            \"html\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"announcement_3\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"index.html\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 0,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 1,\n",
      "          \"number_of_folders\": 0,\n",
      "          \"file_types\": [\n",
      "            \"html\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"announcement_4\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"index.html\",\n",
      "              \"type\": \"file\"\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 0,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 1,\n",
      "          \"number_of_folders\": 0,\n",
      "          \"file_types\": [\n",
      "            \"html\"\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"type\": \"folder\",\n",
      "      \"number_of_subfolders\": 4,\n",
      "      \"number_of_subfiles\": 0,\n",
      "      \"number_of_files\": 0,\n",
      "      \"number_of_folders\": 4,\n",
      "      \"file_types\": [\n",
      "        \"html\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"projects\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"path\": \"2023\",\n",
      "          \"children\": [\n",
      "            {\n",
      "              \"path\": \"proj_1.html\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"proj_2.html\",\n",
      "              \"type\": \"file\"\n",
      "            },\n",
      "            {\n",
      "              \"path\": \"static\",\n",
      "              \"children\": [\n",
      "                {\n",
      "                  \"path\": \"css\",\n",
      "                  \"children\": [\n",
      "                    {\n",
      "                      \"path\": \"bulma-carousel.min.css\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"bulma-slider.min.css\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"bulma.css.map.txt\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"bulma.min.css\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"fontawesome.all.min.css\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"index.css\",\n",
      "                      \"type\": \"file\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"type\": \"folder\",\n",
      "                  \"number_of_subfolders\": 0,\n",
      "                  \"number_of_subfiles\": 0,\n",
      "                  \"number_of_files\": 6,\n",
      "                  \"number_of_folders\": 0,\n",
      "                  \"file_types\": [\n",
      "                    \"txt\",\n",
      "                    \"css\"\n",
      "                  ]\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"images\",\n",
      "                  \"children\": [\n",
      "                    {\n",
      "                      \"path\": \"github_icon.jpg\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"sadtalker.jpg\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"slide_icon.jpg\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"youtube_icon.jpg\",\n",
      "                      \"type\": \"file\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"type\": \"folder\",\n",
      "                  \"number_of_subfolders\": 0,\n",
      "                  \"number_of_subfiles\": 0,\n",
      "                  \"number_of_files\": 4,\n",
      "                  \"number_of_folders\": 0,\n",
      "                  \"file_types\": [\n",
      "                    \"jpg\"\n",
      "                  ]\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"js\",\n",
      "                  \"children\": [\n",
      "                    {\n",
      "                      \"path\": \"bulma-carousel.js\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"bulma-carousel.min.js\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"bulma-slider.js\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"bulma-slider.min.js\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"fontawesome.all.min.js\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"index.js\",\n",
      "                      \"type\": \"file\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"type\": \"folder\",\n",
      "                  \"number_of_subfolders\": 0,\n",
      "                  \"number_of_subfiles\": 0,\n",
      "                  \"number_of_files\": 6,\n",
      "                  \"number_of_folders\": 0,\n",
      "                  \"file_types\": [\n",
      "                    \"js\"\n",
      "                  ]\n",
      "                },\n",
      "                {\n",
      "                  \"path\": \"videos\",\n",
      "                  \"children\": [\n",
      "                    {\n",
      "                      \"path\": \"ablation_ExpNet.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"ablation_facerender.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"chinese_speaker.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"compare.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"comparison_crossid.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"comparisonmp4.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"different_audio_same_style.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"different_style_same_audio.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"eye blinking.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"sadtalker_supp.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"sing.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    },\n",
      "                    {\n",
      "                      \"path\": \"talking.mp4\",\n",
      "                      \"type\": \"file\"\n",
      "                    }\n",
      "                  ],\n",
      "                  \"type\": \"folder\",\n",
      "                  \"number_of_subfolders\": 0,\n",
      "                  \"number_of_subfiles\": 0,\n",
      "                  \"number_of_files\": 12,\n",
      "                  \"number_of_folders\": 0,\n",
      "                  \"file_types\": [\n",
      "                    \"mp4\"\n",
      "                  ]\n",
      "                }\n",
      "              ],\n",
      "              \"type\": \"folder\",\n",
      "              \"number_of_subfolders\": 4,\n",
      "              \"number_of_subfiles\": 0,\n",
      "              \"number_of_files\": 0,\n",
      "              \"number_of_folders\": 4,\n",
      "              \"file_types\": [\n",
      "                \"jpg\",\n",
      "                \"js\",\n",
      "                \"css\",\n",
      "                \"txt\",\n",
      "                \"mp4\"\n",
      "              ]\n",
      "            }\n",
      "          ],\n",
      "          \"type\": \"folder\",\n",
      "          \"number_of_subfolders\": 5,\n",
      "          \"number_of_subfiles\": 0,\n",
      "          \"number_of_files\": 2,\n",
      "          \"number_of_folders\": 1,\n",
      "          \"file_types\": [\n",
      "            \"jpg\",\n",
      "            \"js\",\n",
      "            \"css\",\n",
      "            \"txt\",\n",
      "            \"html\",\n",
      "            \"mp4\"\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"index.html\",\n",
      "          \"type\": \"file\"\n",
      "        },\n",
      "        {\n",
      "          \"path\": \"projects.js\",\n",
      "          \"type\": \"file\"\n",
      "        }\n",
      "      ],\n",
      "      \"type\": \"folder\",\n",
      "      \"number_of_subfolders\": 6,\n",
      "      \"number_of_subfiles\": 0,\n",
      "      \"number_of_files\": 2,\n",
      "      \"number_of_folders\": 1,\n",
      "      \"file_types\": [\n",
      "        \"jpg\",\n",
      "        \"js\",\n",
      "        \"css\",\n",
      "        \"txt\",\n",
      "        \"html\",\n",
      "        \"mp4\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"publications\",\n",
      "      \"children\": [\n",
      "        {\n",
      "          \"path\": \"index.html\",\n",
      "          \"type\": \"file\"\n",
      "        }\n",
      "      ],\n",
      "      \"type\": \"folder\",\n",
      "      \"number_of_subfolders\": 0,\n",
      "      \"number_of_subfiles\": 0,\n",
      "      \"number_of_files\": 1,\n",
      "      \"number_of_folders\": 0,\n",
      "      \"file_types\": [\n",
      "        \"html\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"type\": \"folder\",\n",
      "  \"number_of_subfolders\": 48,\n",
      "  \"number_of_subfiles\": 0,\n",
      "  \"number_of_files\": 9,\n",
      "  \"number_of_folders\": 9,\n",
      "  \"file_types\": [\n",
      "    \"jpg\",\n",
      "    \"js\",\n",
      "    \"css\",\n",
      "    \"LICENSE\",\n",
      "    \"pdf\",\n",
      "    \"cibuild\",\n",
      "    \"Gemfile\",\n",
      "    \"yaml\",\n",
      "    \"md\",\n",
      "    \"txt\",\n",
      "    \"bib\",\n",
      "    \"jpeg\",\n",
      "    \"map\",\n",
      "    \"all-contributorsrc\",\n",
      "    \"deploy\",\n",
      "    \"html\",\n",
      "    \"gitignore\",\n",
      "    \"yml\",\n",
      "    \"png\",\n",
      "    \"dockerignore\",\n",
      "    \"mp4\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "git.enrich_tree() # Adds attributes\n",
    "print(json.dumps(git.tree, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 20:08:59 - Paper - INFO - Extracting paper data...\n",
      "2024-02-23 20:08:59 - Paper - INFO - Per-Pixel Classification is Not All You Need for Semantic Segmentation extracted.\n",
      "2024-02-23 20:08:59 - Paper - INFO - Processing author: Bowen Cheng\n",
      "2024-02-23 20:09:06 - Paper - INFO - Data done: {'name': 'Bowen Cheng', 'affiliation': 'University of Illinois at Urbana-Champaign', 'interests': ['Computer Vision', 'Deep Learning', 'Machine Learning'], 'citedby': 4837, 'citedby5y': 4787, 'hindex': 19, 'hindex5y': 19, 'i10index': 22, 'i10index5y': 21, 'cites_per_year': {2018: 42, 2019: 123, 2020: 275, 2021: 596, 2022: 1134, 2023: 2219, 2024: 433}, 'publications': {'Masked-attention Mask Transformer for Universal Image Segmentation': {'year': '2022', 'citation': 'CVPR 2022, 2022', 'citations': 1024}, 'Per-Pixel Classification is Not All You Need for Semantic Segmentation': {'year': '2021', 'citation': 'NeurIPS 2021, 2021', 'citations': 916}, 'HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation': {'year': '2020', 'citation': 'CVPR 2020, 2020', 'citations': 736}, 'Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation': {'year': '2020', 'citation': 'CVPR 2020, 2020', 'citations': 581}, 'Revisiting RCNN: On Awakening the Classification Power of Faster RCNN': {'year': '2018', 'citation': 'ECCV 2018, 2018', 'citations': 244}, 'Boundary IoU: Improving Object-Centric Image Segmentation Evaluation': {'year': '2021', 'citation': 'CVPR 2021, 2021', 'citations': 224}, 'Naive-Student: Leveraging Semi-Supervised Learning in Video Sequences for Urban Scene Segmentation': {'year': '2020', 'citation': 'ECCV 2020, 2020', 'citations': 188}, 'TS2C: Tight Box Mining with Surrounding Segmentation Context for Weakly Supervised Object Detection': {'year': '2018', 'citation': 'ECCV 2018, 2018', 'citations': 174}, 'SPGNet: Semantic Prediction Guidance for Scene Parsing': {'year': '2019', 'citation': 'ICCV 2019, 2019', 'citations': 124}, 'Mask2Former for Video Instance Segmentation': {'year': '2021', 'citation': 'arXiv, 2021', 'citations': 118}, 'SkyNet: a Hardware-Efficient Method for Object Detection and Tracking on Embedded Systems': {'year': '2019', 'citation': 'MLSys (Conference on Machine Learning and Systems), 2019', 'citations': 97}, 'Pointly-Supervised Instance Segmentation': {'year': '2022', 'citation': 'CVPR 2022, 2022', 'citations': 94}, 'Panoptic-DeepLab': {'year': '2019', 'citation': 'ICCV 2019 COCO Workshop, 2019', 'citations': 60}, 'Enhance Visual Recognition under Adverse Conditions via Deep Networks': {'year': '2017', 'citation': 'IEEE Transactions on Image Processing (TIP), 2017', 'citations': 53}, 'Decoupled Classification Refinement: Hard False Positive Suppression for Object Detection': {'year': '2018', 'citation': 'arXiv, 2018', 'citations': 47}, 'Deformable Convolutional Networks–COCO Detection and Segmentation Challenge 2017 Entry': {'year': '2017', 'citation': 'ICCV 2017 COCO Workshop, 2017', 'citations': 41}, 'Robust Emotion Recognition from Low Quality and Low Bit Rate Video: A Deep Learning Approach': {'year': '2017', 'citation': 'ACII 2017, 2017', 'citations': 36}, 'High Frequency Residual Learning for Multi-Scale Image Classification': {'year': '2019', 'citation': 'BMVC 2019, 2019', 'citations': 23}, 'Pseudo-IoU: Improving Label Assignment in Anchor-Free Object Detection': {'year': '2021', 'citation': 'CVPRW 2021, 2021', 'citations': 21}, 'A Simple Non-iid Sampling Approach for Efficient Training and Better Generalization': {'year': '2018', 'citation': 'arXiv, 2018', 'citations': 12}, 'ScaleNAS: Multi-Path One-Shot NAS for Scale-Aware High-Resolution Representation': {'year': '2022', 'citation': 'International Conference on Automated Machine Learning, 15/1-18, 2022', 'citations': 11}, 'Visual recognition in very low-quality settings: Delving into the power of pre-training': {'year': '2018', 'citation': 'AAAI 2019, 2018', 'citations': 11}, 'Locating Noise is Halfway Denoising for Semi-Supervised Segmentation': {'year': '2023', 'citation': 'Proceedings of the IEEE/CVF International Conference on Computer Vision …, 2023', 'citations': 2}, 'From pixels to regions: Toward universal image segmentation': {'year': '2022', 'citation': 'University of Illinois at Urbana-Champaign, 2022', 'citations': 0}}}\n",
      "2024-02-23 20:09:06 - Paper - INFO - Bowen Cheng processed.\n",
      "2024-02-23 20:09:06 - Paper - INFO - Processing author: Alexander G. Schwing\n",
      "2024-02-23 20:09:07 - Paper - ERROR - Error processing author Alexander G. Schwing. Error: \n",
      "2024-02-23 20:09:07 - Paper - INFO - Alexander G. Schwing processed.\n",
      "2024-02-23 20:09:07 - Paper - INFO - Processing author: Alexander Kirillov\n",
      "2024-02-23 20:09:14 - Paper - INFO - Data done: {'name': 'Alexander Kirillov', 'affiliation': 'Research Scientist, Facebook AI Research (FAIR)', 'interests': ['computer vision', 'machine learning', 'deep learning'], 'citedby': 23492, 'citedby5y': 23201, 'hindex': 30, 'hindex5y': 27, 'i10index': 32, 'i10index5y': 30, 'cites_per_year': {2017: 60, 2018: 181, 2019: 340, 2020: 957, 2021: 2887, 2022: 5873, 2023: 10933, 2024: 2185}, 'publications': {'End-to-end object detection with transformers': {'year': '2020', 'citation': 'European conference on computer vision, 213-229, 2020', 'citations': 10040}, 'Detectron2': {'year': '2019', 'citation': '', 'citations': 2553}, 'Segment anything': {'year': '2023', 'citation': 'arXiv preprint arXiv:2304.02643, 2023', 'citations': 2218}, 'Panoptic segmentation': {'year': '2019', 'citation': 'Proceedings of the IEEE/CVF conference on computer vision and pattern …, 2019', 'citations': 1436}, 'Panoptic Feature Pyramid Networks': {'year': '2019', 'citation': 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern …, 2019', 'citations': 1229}, 'Masked-attention mask transformer for universal image segmentation': {'year': '2022', 'citation': 'Proceedings of the IEEE/CVF conference on computer vision and pattern …, 2022', 'citations': 1024}, 'Per-pixel classification is not all you need for semantic segmentation': {'year': '2021', 'citation': 'Advances in Neural Information Processing Systems 34, 17864-17875, 2021', 'citations': 916}, 'Pointrend: Image segmentation as rendering': {'year': '2020', 'citation': 'Proceedings of the IEEE/CVF conference on computer vision and pattern …, 2020', 'citations': 852}, 'Trackformer: Multi-object tracking with transformers': {'year': '2022', 'citation': 'Proceedings of the IEEE/CVF conference on computer vision and pattern …, 2022', 'citations': 607}, 'Exploring randomly wired neural networks for image recognition': {'year': '2019', 'citation': 'Proceedings of the IEEE/CVF International Conference on Computer Vision …, 2019', 'citations': 417}, 'InstanceCut: from Edges to Instances with MultiCut': {'year': '2017', 'citation': 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017', 'citations': 302}, 'Slip: Self-supervision meets language-image pre-training': {'year': '2022', 'citation': 'European Conference on Computer Vision, 529-544, 2022', 'citations': 282}, 'Boundary IoU: Improving object-centric image segmentation evaluation': {'year': '2021', 'citation': 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern …, 2021', 'citations': 224}, 'Conditional random fields meet deep neural networks for semantic segmentation: Combining probabilistic graphical models with deep learning for structured prediction': {'year': '2018', 'citation': 'IEEE Signal Processing Magazine 35 (1), 37-52, 2018', 'citations': 161}, 'Global hypothesis generation for 6D object pose estimation': {'year': '2017', 'citation': 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017', 'citations': 144}, 'Joint Graph Decomposition & Node Labeling: Problem, Algorithms, Applications': {'year': '2017', 'citation': 'IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017', 'citations': 125}, 'Mask2former for video instance segmentation': {'year': '2021', 'citation': 'arXiv preprint arXiv:2112.10764, 2021', 'citations': 118}, 'Segment anything. arXiv 2023': {'year': 'N/A', 'citation': 'arXiv preprint arXiv:2304.02643, 0', 'citations': 103}, 'Pointly-supervised instance segmentation': {'year': '2022', 'citation': 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern …, 2022', 'citations': 94}, 'End-to-end object detection with transformers. arXiv 2020': {'year': '2005', 'citation': 'arXiv preprint arXiv:2005.12872, 2005', 'citations': 81}, 'Analyzing Modular CNN Architectures for Joint Depth Prediction and Semantic Segmentation': {'year': '2017', 'citation': 'IEEE International Conference on Robotics and Automation (ICRA), 2017, 2017', 'citations': 79}, 'On interaction between augmentations and corruptions in natural corruption robustness': {'year': '2021', 'citation': 'Advances in Neural Information Processing Systems 34, 3571-3583, 2021', 'citations': 72}, 'European conference on computer vision': {'year': '2020', 'citation': 'Springer, 2020', 'citations': 65}, 'A unified architecture for instance and semantic segmentation': {'year': '2017', 'citation': 'CVPR, 2017', 'citations': 51}, 'Inferring M-Best Diverse Labelings in a Single One': {'year': '2015', 'citation': 'The IEEE International Conference on Computer Vision (ICCV), 2015', 'citations': 49}, 'Joint Training of Generic CNN-CRF Models with Stochastic Optimization': {'year': '2016', 'citation': 'The 13th Asian Conference on Computer Vision (ACCV), 2016', 'citations': 47}, 'Point-level region contrast for object detection pre-training': {'year': '2022', 'citation': 'Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern …, 2022', 'citations': 43}, 'Evaluating large-vocabulary object detectors: The devil is in the details': {'year': '2021', 'citation': 'arXiv preprint arXiv:2102.01066, 2021', 'citations': 41}, 'End-to-end object detection with transformers (2020)': {'year': '2005', 'citation': 'URL https://arxiv. org/abs, 2005', 'citations': 33}, 'A Comparative Study of Local Search Algorithms for Correlation Clustering': {'year': '2017', 'citation': 'German Conference on Pattern Recognition (GCPR), 103-114, 2017', 'citations': 31}, 'M-Best-Diverse Labelings for Submodular Energies and Beyond': {'year': '2015', 'citation': 'Advances in Neural Information Processing Systems (NIPS), 613-621, 2015', 'citations': 23}, 'Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization': {'year': '2016', 'citation': 'Advances in Neural Information Processing Systems (NIPS), 334-342, 2016', 'citations': 15}, 'Recognizing scenes from novel viewpoints': {'year': '2021', 'citation': 'arXiv preprint arXiv:2112.01520, 2021', 'citations': 6}, 'Linear combination of random forests for the Relevance Prediction Challenge': {'year': '2012', 'citation': 'Proc. of Int. Conf. on Web Service and Data Mining workshop on Web Search …, 2012', 'citations': 6}, 'R-MAE: Regions Meet Masked Autoencoders': {'year': '2023', 'citation': 'arXiv preprint arXiv:2306.05411, 2023', 'citations': 3}, 'Deep Part-Based Generative Shape Model with Latent Variables': {'year': '2016', 'citation': 'British Machine Vision Conference (BMVC), 2016', 'citations': 2}, 'Exploring Aspects of Image Segmentation: Diversity, Global Reasoning, and Panoptic Formulation.': {'year': '2018', 'citation': 'University of Heidelberg, Germany, 2018', 'citations': 0}, 'Supplemental Material: On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness': {'year': 'N/A', 'citation': '', 'citations': 0}, 'Supplementary Materials: Per-Pixel Classification is Not All You Need for Semantic Segmentation': {'year': 'N/A', 'citation': 'training 12, 0.2, 0', 'citations': 0}, 'Supplementary Materials: Pointly-Supervised Instance Segmentation': {'year': 'N/A', 'citation': 'Annotation 100, 300, 0', 'citations': 0}, 'Supplementary Materials: Masked-attention Mask Transformer for Universal Image Segmentation': {'year': 'N/A', 'citation': '', 'citations': 0}, 'TrackFormer: Multi-Object Tracking with Transformers Supplementary Materials': {'year': 'N/A', 'citation': '', 'citations': 0}, 'Supplementary Materials: Boundary IoU: Improving Object-Centric Image Segmentation Evaluation': {'year': 'N/A', 'citation': '', 'citations': 0}, 'Supplementary Materials: Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization': {'year': 'N/A', 'citation': '', 'citations': 0}}}\n",
      "2024-02-23 20:09:14 - Paper - INFO - Alexander Kirillov processed.\n",
      "2024-02-23 20:09:14 - Paper - INFO - Extraction and processing completed.\n"
     ]
    }
   ],
   "source": [
    "paper.get_paper() # Gets paper from arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Per-Pixel Classification is Not All You Need for Semantic Segmentation',\n",
       " 'authors': ['Bowen Cheng', 'Alexander G. Schwing', 'Alexander Kirillov'],\n",
       " 'summary': 'Modern approaches typically formulate semantic segmentation as a per-pixel\\nclassification task, while instance-level segmentation is handled with an\\nalternative mask classification. Our key insight: mask classification is\\nsufficiently general to solve both semantic- and instance-level segmentation\\ntasks in a unified manner using the exact same model, loss, and training\\nprocedure. Following this observation, we propose MaskFormer, a simple mask\\nclassification model which predicts a set of binary masks, each associated with\\na single global class label prediction. Overall, the proposed mask\\nclassification-based method simplifies the landscape of effective approaches to\\nsemantic and panoptic segmentation tasks and shows excellent empirical results.\\nIn particular, we observe that MaskFormer outperforms per-pixel classification\\nbaselines when the number of classes is large. Our mask classification-based\\nmethod outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K)\\nand panoptic segmentation (52.7 PQ on COCO) models.',\n",
       " 'published': '07/13/2021',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2107.06278v2',\n",
       " 'citations': 916}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.paper_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"Per-Pixel Classification is Not All You Need for Semantic Segmentation\",\n",
      "  \"authors\": [\n",
      "    \"Bowen Cheng\",\n",
      "    \"Alexander G. Schwing\",\n",
      "    \"Alexander Kirillov\"\n",
      "  ],\n",
      "  \"summary\": \"Modern approaches typically formulate semantic segmentation as a per-pixel\\nclassification task, while instance-level segmentation is handled with an\\nalternative mask classification. Our key insight: mask classification is\\nsufficiently general to solve both semantic- and instance-level segmentation\\ntasks in a unified manner using the exact same model, loss, and training\\nprocedure. Following this observation, we propose MaskFormer, a simple mask\\nclassification model which predicts a set of binary masks, each associated with\\na single global class label prediction. Overall, the proposed mask\\nclassification-based method simplifies the landscape of effective approaches to\\nsemantic and panoptic segmentation tasks and shows excellent empirical results.\\nIn particular, we observe that MaskFormer outperforms per-pixel classification\\nbaselines when the number of classes is large. Our mask classification-based\\nmethod outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K)\\nand panoptic segmentation (52.7 PQ on COCO) models.\",\n",
      "  \"published\": \"07/13/2021\",\n",
      "  \"pdf_url\": \"http://arxiv.org/pdf/2107.06278v2\",\n",
      "  \"citations\": 916\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(paper.paper_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Bowen Cheng\": {\n",
      "    \"name\": \"Bowen Cheng\",\n",
      "    \"affiliation\": \"University of Illinois at Urbana-Champaign\",\n",
      "    \"interests\": [\n",
      "      \"Computer Vision\",\n",
      "      \"Deep Learning\",\n",
      "      \"Machine Learning\"\n",
      "    ],\n",
      "    \"citedby\": 4837,\n",
      "    \"citedby5y\": 4787,\n",
      "    \"hindex\": 19,\n",
      "    \"hindex5y\": 19,\n",
      "    \"i10index\": 22,\n",
      "    \"i10index5y\": 21,\n",
      "    \"cites_per_year\": {\n",
      "      \"2018\": 42,\n",
      "      \"2019\": 123,\n",
      "      \"2020\": 275,\n",
      "      \"2021\": 596,\n",
      "      \"2022\": 1134,\n",
      "      \"2023\": 2219,\n",
      "      \"2024\": 433\n",
      "    },\n",
      "    \"publications\": {\n",
      "      \"Masked-attention Mask Transformer for Universal Image Segmentation\": {\n",
      "        \"year\": \"2022\",\n",
      "        \"citation\": \"CVPR 2022, 2022\",\n",
      "        \"citations\": 1024\n",
      "      },\n",
      "      \"Per-Pixel Classification is Not All You Need for Semantic Segmentation\": {\n",
      "        \"year\": \"2021\",\n",
      "        \"citation\": \"NeurIPS 2021, 2021\",\n",
      "        \"citations\": 916\n",
      "      },\n",
      "      \"HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation\": {\n",
      "        \"year\": \"2020\",\n",
      "        \"citation\": \"CVPR 2020, 2020\",\n",
      "        \"citations\": 736\n",
      "      },\n",
      "      \"Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation\": {\n",
      "        \"year\": \"2020\",\n",
      "        \"citation\": \"CVPR 2020, 2020\",\n",
      "        \"citations\": 581\n",
      "      },\n",
      "      \"Revisiting RCNN: On Awakening the Classification Power of Faster RCNN\": {\n",
      "        \"year\": \"2018\",\n",
      "        \"citation\": \"ECCV 2018, 2018\",\n",
      "        \"citations\": 244\n",
      "      },\n",
      "      \"Boundary IoU: Improving Object-Centric Image Segmentation Evaluation\": {\n",
      "        \"year\": \"2021\",\n",
      "        \"citation\": \"CVPR 2021, 2021\",\n",
      "        \"citations\": 224\n",
      "      },\n",
      "      \"Naive-Student: Leveraging Semi-Supervised Learning in Video Sequences for Urban Scene Segmentation\": {\n",
      "        \"year\": \"2020\",\n",
      "        \"citation\": \"ECCV 2020, 2020\",\n",
      "        \"citations\": 188\n",
      "      },\n",
      "      \"TS2C: Tight Box Mining with Surrounding Segmentation Context for Weakly Supervised Object Detection\": {\n",
      "        \"year\": \"2018\",\n",
      "        \"citation\": \"ECCV 2018, 2018\",\n",
      "        \"citations\": 174\n",
      "      },\n",
      "      \"SPGNet: Semantic Prediction Guidance for Scene Parsing\": {\n",
      "        \"year\": \"2019\",\n",
      "        \"citation\": \"ICCV 2019, 2019\",\n",
      "        \"citations\": 124\n",
      "      },\n",
      "      \"Mask2Former for Video Instance Segmentation\": {\n",
      "        \"year\": \"2021\",\n",
      "        \"citation\": \"arXiv, 2021\",\n",
      "        \"citations\": 118\n",
      "      },\n",
      "      \"SkyNet: a Hardware-Efficient Method for Object Detection and Tracking on Embedded Systems\": {\n",
      "        \"year\": \"2019\",\n",
      "        \"citation\": \"MLSys (Conference on Machine Learning and Systems), 2019\",\n",
      "        \"citations\": 97\n",
      "      },\n",
      "      \"Pointly-Supervised Instance Segmentation\": {\n",
      "        \"year\": \"2022\",\n",
      "        \"citation\": \"CVPR 2022, 2022\",\n",
      "        \"citations\": 94\n",
      "      },\n",
      "      \"Panoptic-DeepLab\": {\n",
      "        \"year\": \"2019\",\n",
      "        \"citation\": \"ICCV 2019 COCO Workshop, 2019\",\n",
      "        \"citations\": 60\n",
      "      },\n",
      "      \"Enhance Visual Recognition under Adverse Conditions via Deep Networks\": {\n",
      "        \"year\": \"2017\",\n",
      "        \"citation\": \"IEEE Transactions on Image Processing (TIP), 2017\",\n",
      "        \"citations\": 53\n",
      "      },\n",
      "      \"Decoupled Classification Refinement: Hard False Positive Suppression for Object Detection\": {\n",
      "        \"year\": \"2018\",\n",
      "        \"citation\": \"arXiv, 2018\",\n",
      "        \"citations\": 47\n",
      "      },\n",
      "      \"Deformable Convolutional Networks\\u2013COCO Detection and Segmentation Challenge 2017 Entry\": {\n",
      "        \"year\": \"2017\",\n",
      "        \"citation\": \"ICCV 2017 COCO Workshop, 2017\",\n",
      "        \"citations\": 41\n",
      "      },\n",
      "      \"Robust Emotion Recognition from Low Quality and Low Bit Rate Video: A Deep Learning Approach\": {\n",
      "        \"year\": \"2017\",\n",
      "        \"citation\": \"ACII 2017, 2017\",\n",
      "        \"citations\": 36\n",
      "      },\n",
      "      \"High Frequency Residual Learning for Multi-Scale Image Classification\": {\n",
      "        \"year\": \"2019\",\n",
      "        \"citation\": \"BMVC 2019, 2019\",\n",
      "        \"citations\": 23\n",
      "      },\n",
      "      \"Pseudo-IoU: Improving Label Assignment in Anchor-Free Object Detection\": {\n",
      "        \"year\": \"2021\",\n",
      "        \"citation\": \"CVPRW 2021, 2021\",\n",
      "        \"citations\": 21\n",
      "      },\n",
      "      \"A Simple Non-iid Sampling Approach for Efficient Training and Better Generalization\": {\n",
      "        \"year\": \"2018\",\n",
      "        \"citation\": \"arXiv, 2018\",\n",
      "        \"citations\": 12\n",
      "      },\n",
      "      \"ScaleNAS: Multi-Path One-Shot NAS for Scale-Aware High-Resolution Representation\": {\n",
      "        \"year\": \"2022\",\n",
      "        \"citation\": \"International Conference on Automated Machine Learning, 15/1-18, 2022\",\n",
      "        \"citations\": 11\n",
      "      },\n",
      "      \"Visual recognition in very low-quality settings: Delving into the power of pre-training\": {\n",
      "        \"year\": \"2018\",\n",
      "        \"citation\": \"AAAI 2019, 2018\",\n",
      "        \"citations\": 11\n",
      "      },\n",
      "      \"Locating Noise is Halfway Denoising for Semi-Supervised Segmentation\": {\n",
      "        \"year\": \"2023\",\n",
      "        \"citation\": \"Proceedings of the IEEE/CVF International Conference on Computer Vision \\u2026, 2023\",\n",
      "        \"citations\": 2\n",
      "      },\n",
      "      \"From pixels to regions: Toward universal image segmentation\": {\n",
      "        \"year\": \"2022\",\n",
      "        \"citation\": \"University of Illinois at Urbana-Champaign, 2022\",\n",
      "        \"citations\": 0\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"Alexander G. Schwing\": null,\n",
      "  \"Alexander Kirillov\": {\n",
      "    \"name\": \"Alexander Kirillov\",\n",
      "    \"affiliation\": \"Research Scientist, Facebook AI Research (FAIR)\",\n",
      "    \"interests\": [\n",
      "      \"computer vision\",\n",
      "      \"machine learning\",\n",
      "      \"deep learning\"\n",
      "    ],\n",
      "    \"citedby\": 23492,\n",
      "    \"citedby5y\": 23201,\n",
      "    \"hindex\": 30,\n",
      "    \"hindex5y\": 27,\n",
      "    \"i10index\": 32,\n",
      "    \"i10index5y\": 30,\n",
      "    \"cites_per_year\": {\n",
      "      \"2017\": 60,\n",
      "      \"2018\": 181,\n",
      "      \"2019\": 340,\n",
      "      \"2020\": 957,\n",
      "      \"2021\": 2887,\n",
      "      \"2022\": 5873,\n",
      "      \"2023\": 10933,\n",
      "      \"2024\": 2185\n",
      "    },\n",
      "    \"publications\": {\n",
      "      \"End-to-end object detection with transformers\": {\n",
      "        \"year\": \"2020\",\n",
      "        \"citation\": \"European conference on computer vision, 213-229, 2020\",\n",
      "        \"citations\": 10040\n",
      "      },\n",
      "      \"Detectron2\": {\n",
      "        \"year\": \"2019\",\n",
      "        \"citation\": \"\",\n",
      "        \"citations\": 2553\n",
      "      },\n",
      "      \"Segment anything\": {\n",
      "        \"year\": \"2023\",\n",
      "        \"citation\": \"arXiv preprint arXiv:2304.02643, 2023\",\n",
      "        \"citations\": 2218\n",
      "      },\n",
      "      \"Panoptic segmentation\": {\n",
      "        \"year\": \"2019\",\n",
      "        \"citation\": \"Proceedings of the IEEE/CVF conference on computer vision and pattern \\u2026, 2019\",\n",
      "        \"citations\": 1436\n",
      "      },\n",
      "      \"Panoptic Feature Pyramid Networks\": {\n",
      "        \"year\": \"2019\",\n",
      "        \"citation\": \"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern \\u2026, 2019\",\n",
      "        \"citations\": 1229\n",
      "      },\n",
      "      \"Masked-attention mask transformer for universal image segmentation\": {\n",
      "        \"year\": \"2022\",\n",
      "        \"citation\": \"Proceedings of the IEEE/CVF conference on computer vision and pattern \\u2026, 2022\",\n",
      "        \"citations\": 1024\n",
      "      },\n",
      "      \"Per-pixel classification is not all you need for semantic segmentation\": {\n",
      "        \"year\": \"2021\",\n",
      "        \"citation\": \"Advances in Neural Information Processing Systems 34, 17864-17875, 2021\",\n",
      "        \"citations\": 916\n",
      "      },\n",
      "      \"Pointrend: Image segmentation as rendering\": {\n",
      "        \"year\": \"2020\",\n",
      "        \"citation\": \"Proceedings of the IEEE/CVF conference on computer vision and pattern \\u2026, 2020\",\n",
      "        \"citations\": 852\n",
      "      },\n",
      "      \"Trackformer: Multi-object tracking with transformers\": {\n",
      "        \"year\": \"2022\",\n",
      "        \"citation\": \"Proceedings of the IEEE/CVF conference on computer vision and pattern \\u2026, 2022\",\n",
      "        \"citations\": 607\n",
      "      },\n",
      "      \"Exploring randomly wired neural networks for image recognition\": {\n",
      "        \"year\": \"2019\",\n",
      "        \"citation\": \"Proceedings of the IEEE/CVF International Conference on Computer Vision \\u2026, 2019\",\n",
      "        \"citations\": 417\n",
      "      },\n",
      "      \"InstanceCut: from Edges to Instances with MultiCut\": {\n",
      "        \"year\": \"2017\",\n",
      "        \"citation\": \"IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017\",\n",
      "        \"citations\": 302\n",
      "      },\n",
      "      \"Slip: Self-supervision meets language-image pre-training\": {\n",
      "        \"year\": \"2022\",\n",
      "        \"citation\": \"European Conference on Computer Vision, 529-544, 2022\",\n",
      "        \"citations\": 282\n",
      "      },\n",
      "      \"Boundary IoU: Improving object-centric image segmentation evaluation\": {\n",
      "        \"year\": \"2021\",\n",
      "        \"citation\": \"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern \\u2026, 2021\",\n",
      "        \"citations\": 224\n",
      "      },\n",
      "      \"Conditional random fields meet deep neural networks for semantic segmentation: Combining probabilistic graphical models with deep learning for structured prediction\": {\n",
      "        \"year\": \"2018\",\n",
      "        \"citation\": \"IEEE Signal Processing Magazine 35 (1), 37-52, 2018\",\n",
      "        \"citations\": 161\n",
      "      },\n",
      "      \"Global hypothesis generation for 6D object pose estimation\": {\n",
      "        \"year\": \"2017\",\n",
      "        \"citation\": \"IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017\",\n",
      "        \"citations\": 144\n",
      "      },\n",
      "      \"Joint Graph Decomposition & Node Labeling: Problem, Algorithms, Applications\": {\n",
      "        \"year\": \"2017\",\n",
      "        \"citation\": \"IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017\",\n",
      "        \"citations\": 125\n",
      "      },\n",
      "      \"Mask2former for video instance segmentation\": {\n",
      "        \"year\": \"2021\",\n",
      "        \"citation\": \"arXiv preprint arXiv:2112.10764, 2021\",\n",
      "        \"citations\": 118\n",
      "      },\n",
      "      \"Segment anything. arXiv 2023\": {\n",
      "        \"year\": \"N/A\",\n",
      "        \"citation\": \"arXiv preprint arXiv:2304.02643, 0\",\n",
      "        \"citations\": 103\n",
      "      },\n",
      "      \"Pointly-supervised instance segmentation\": {\n",
      "        \"year\": \"2022\",\n",
      "        \"citation\": \"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern \\u2026, 2022\",\n",
      "        \"citations\": 94\n",
      "      },\n",
      "      \"End-to-end object detection with transformers. arXiv 2020\": {\n",
      "        \"year\": \"2005\",\n",
      "        \"citation\": \"arXiv preprint arXiv:2005.12872, 2005\",\n",
      "        \"citations\": 81\n",
      "      },\n",
      "      \"Analyzing Modular CNN Architectures for Joint Depth Prediction and Semantic Segmentation\": {\n",
      "        \"year\": \"2017\",\n",
      "        \"citation\": \"IEEE International Conference on Robotics and Automation (ICRA), 2017, 2017\",\n",
      "        \"citations\": 79\n",
      "      },\n",
      "      \"On interaction between augmentations and corruptions in natural corruption robustness\": {\n",
      "        \"year\": \"2021\",\n",
      "        \"citation\": \"Advances in Neural Information Processing Systems 34, 3571-3583, 2021\",\n",
      "        \"citations\": 72\n",
      "      },\n",
      "      \"European conference on computer vision\": {\n",
      "        \"year\": \"2020\",\n",
      "        \"citation\": \"Springer, 2020\",\n",
      "        \"citations\": 65\n",
      "      },\n",
      "      \"A unified architecture for instance and semantic segmentation\": {\n",
      "        \"year\": \"2017\",\n",
      "        \"citation\": \"CVPR, 2017\",\n",
      "        \"citations\": 51\n",
      "      },\n",
      "      \"Inferring M-Best Diverse Labelings in a Single One\": {\n",
      "        \"year\": \"2015\",\n",
      "        \"citation\": \"The IEEE International Conference on Computer Vision (ICCV), 2015\",\n",
      "        \"citations\": 49\n",
      "      },\n",
      "      \"Joint Training of Generic CNN-CRF Models with Stochastic Optimization\": {\n",
      "        \"year\": \"2016\",\n",
      "        \"citation\": \"The 13th Asian Conference on Computer Vision (ACCV), 2016\",\n",
      "        \"citations\": 47\n",
      "      },\n",
      "      \"Point-level region contrast for object detection pre-training\": {\n",
      "        \"year\": \"2022\",\n",
      "        \"citation\": \"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern \\u2026, 2022\",\n",
      "        \"citations\": 43\n",
      "      },\n",
      "      \"Evaluating large-vocabulary object detectors: The devil is in the details\": {\n",
      "        \"year\": \"2021\",\n",
      "        \"citation\": \"arXiv preprint arXiv:2102.01066, 2021\",\n",
      "        \"citations\": 41\n",
      "      },\n",
      "      \"End-to-end object detection with transformers (2020)\": {\n",
      "        \"year\": \"2005\",\n",
      "        \"citation\": \"URL https://arxiv. org/abs, 2005\",\n",
      "        \"citations\": 33\n",
      "      },\n",
      "      \"A Comparative Study of Local Search Algorithms for Correlation Clustering\": {\n",
      "        \"year\": \"2017\",\n",
      "        \"citation\": \"German Conference on Pattern Recognition (GCPR), 103-114, 2017\",\n",
      "        \"citations\": 31\n",
      "      },\n",
      "      \"M-Best-Diverse Labelings for Submodular Energies and Beyond\": {\n",
      "        \"year\": \"2015\",\n",
      "        \"citation\": \"Advances in Neural Information Processing Systems (NIPS), 613-621, 2015\",\n",
      "        \"citations\": 23\n",
      "      },\n",
      "      \"Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization\": {\n",
      "        \"year\": \"2016\",\n",
      "        \"citation\": \"Advances in Neural Information Processing Systems (NIPS), 334-342, 2016\",\n",
      "        \"citations\": 15\n",
      "      },\n",
      "      \"Recognizing scenes from novel viewpoints\": {\n",
      "        \"year\": \"2021\",\n",
      "        \"citation\": \"arXiv preprint arXiv:2112.01520, 2021\",\n",
      "        \"citations\": 6\n",
      "      },\n",
      "      \"Linear combination of random forests for the Relevance Prediction Challenge\": {\n",
      "        \"year\": \"2012\",\n",
      "        \"citation\": \"Proc. of Int. Conf. on Web Service and Data Mining workshop on Web Search \\u2026, 2012\",\n",
      "        \"citations\": 6\n",
      "      },\n",
      "      \"R-MAE: Regions Meet Masked Autoencoders\": {\n",
      "        \"year\": \"2023\",\n",
      "        \"citation\": \"arXiv preprint arXiv:2306.05411, 2023\",\n",
      "        \"citations\": 3\n",
      "      },\n",
      "      \"Deep Part-Based Generative Shape Model with Latent Variables\": {\n",
      "        \"year\": \"2016\",\n",
      "        \"citation\": \"British Machine Vision Conference (BMVC), 2016\",\n",
      "        \"citations\": 2\n",
      "      },\n",
      "      \"Exploring Aspects of Image Segmentation: Diversity, Global Reasoning, and Panoptic Formulation.\": {\n",
      "        \"year\": \"2018\",\n",
      "        \"citation\": \"University of Heidelberg, Germany, 2018\",\n",
      "        \"citations\": 0\n",
      "      },\n",
      "      \"Supplemental Material: On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness\": {\n",
      "        \"year\": \"N/A\",\n",
      "        \"citation\": \"\",\n",
      "        \"citations\": 0\n",
      "      },\n",
      "      \"Supplementary Materials: Per-Pixel Classification is Not All You Need for Semantic Segmentation\": {\n",
      "        \"year\": \"N/A\",\n",
      "        \"citation\": \"training 12, 0.2, 0\",\n",
      "        \"citations\": 0\n",
      "      },\n",
      "      \"Supplementary Materials: Pointly-Supervised Instance Segmentation\": {\n",
      "        \"year\": \"N/A\",\n",
      "        \"citation\": \"Annotation 100, 300, 0\",\n",
      "        \"citations\": 0\n",
      "      },\n",
      "      \"Supplementary Materials: Masked-attention Mask Transformer for Universal Image Segmentation\": {\n",
      "        \"year\": \"N/A\",\n",
      "        \"citation\": \"\",\n",
      "        \"citations\": 0\n",
      "      },\n",
      "      \"TrackFormer: Multi-Object Tracking with Transformers Supplementary Materials\": {\n",
      "        \"year\": \"N/A\",\n",
      "        \"citation\": \"\",\n",
      "        \"citations\": 0\n",
      "      },\n",
      "      \"Supplementary Materials: Boundary IoU: Improving Object-Centric Image Segmentation Evaluation\": {\n",
      "        \"year\": \"N/A\",\n",
      "        \"citation\": \"\",\n",
      "        \"citations\": 0\n",
      "      },\n",
      "      \"Supplementary Materials: Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization\": {\n",
      "        \"year\": \"N/A\",\n",
      "        \"citation\": \"\",\n",
      "        \"citations\": 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(paper.author_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 20:09:14 - Paper - INFO - Downloading paper...\n",
      "2024-02-23 20:09:14 - Paper - INFO - Downloaded.\n",
      "2024-02-23 20:09:14 - Paper - INFO - Saving paper metadata...\n",
      "2024-02-23 20:09:14 - Paper - INFO - Metadata saved.\n",
      "2024-02-23 20:09:14 - Paper - INFO - Saving author metadata...\n",
      "2024-02-23 20:09:14 - Paper - INFO - Author metadata saved.\n",
      "2024-02-23 20:09:14 - Paper - INFO - Author metadata saved.\n",
      "2024-02-23 20:09:14 - Paper - INFO - Author metadata saved.\n",
      "2024-02-23 20:09:14 - Github - INFO - Saving tree...\n",
      "2024-02-23 20:09:14 - Github - INFO - Tree saved.\n",
      "2024-02-23 20:09:14 - Github - INFO - Saving metadata...\n",
      "2024-02-23 20:09:14 - Github - INFO - Metadata saved.\n",
      "2024-02-23 20:09:14 - Github - INFO - Saving commit history...\n",
      "2024-02-23 20:09:14 - Github - INFO - Commit history saved.\n",
      "2024-02-23 20:09:14 - Github - INFO - Saving tree (formatted)...\n",
      "2024-02-23 20:09:14 - Github - INFO - Tree (formatted) saved.\n",
      "2024-02-23 20:09:14 - Github - INFO - Saving owner...\n",
      "2024-02-23 20:09:14 - Github - INFO - Owner saved.\n",
      "2024-02-23 20:09:14 - Github - INFO - Saving members...\n",
      "2024-02-23 20:09:14 - Github - INFO - Vishal-S-P saved.\n",
      "2024-02-23 20:09:14 - Github - INFO - JyotiP10 saved.\n",
      "2024-02-23 20:09:14 - Github - INFO - Saved members...\n"
     ]
    }
   ],
   "source": [
    "depot.save(paper, git) # Saves paper and repo to depot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paper_metadata': {'title': 'PortaSpeech: Portable and High-Quality Generative Text-to-Speech', 'authors': ['Yi Ren', 'Jinglin Liu', 'Zhou Zhao'], 'summary': 'Non-autoregressive text-to-speech (NAR-TTS) models such as FastSpeech 2 and\\nGlow-TTS can synthesize high-quality speech from the given text in parallel.\\nAfter analyzing two kinds of generative NAR-TTS models (VAE and normalizing\\nflow), we find that: VAE is good at capturing the long-range semantics features\\n(e.g., prosody) even with small model size but suffers from blurry and\\nunnatural results; and normalizing flow is good at reconstructing the frequency\\nbin-wise details but performs poorly when the number of model parameters is\\nlimited. Inspired by these observations, to generate diverse speech with\\nnatural details and rich prosody using a lightweight architecture, we propose\\nPortaSpeech, a portable and high-quality generative text-to-speech model.\\nSpecifically, 1) to model both the prosody and mel-spectrogram details\\naccurately, we adopt a lightweight VAE with an enhanced prior followed by a\\nflow-based post-net with strong conditional inputs as the main architecture. 2)\\nTo further compress the model size and memory footprint, we introduce the\\ngrouped parameter sharing mechanism to the affine coupling layers in the\\npost-net. 3) To improve the expressiveness of synthesized speech and reduce the\\ndependency on accurate fine-grained alignment between text and speech, we\\npropose a linguistic encoder with mixture alignment combining hard inter-word\\nalignment and soft intra-word alignment, which explicitly extracts word-level\\nsemantic information. Experimental results show that PortaSpeech outperforms\\nother TTS models in both voice quality and prosody modeling in terms of\\nsubjective and objective evaluation metrics, and shows only a slight\\nperformance degradation when reducing the model parameters to 6.7M (about 4x\\nmodel size and 3x runtime memory compression ratio compared with FastSpeech 2).\\nOur extensive ablation studies demonstrate that each design in PortaSpeech is\\neffective.', 'published': '09/30/2021', 'pdf_url': 'http://arxiv.org/pdf/2109.15166v5'}, 'author_data': [{'name': 'Yi Ren', 'interests': ['cell biology'], 'citedby': 8743, 'citedby5y': 2908, 'hindex': 43, 'hindex5y': 29, 'i10index': 72, 'i10index5y': 54}, {'name': 'Jinglin Liu', 'interests': ['plasma catalysis', 'gliding arc', 'methane pyrolysis', 'air pollution control', 'plasma chemistry'], 'citedby': 1986, 'citedby5y': 1403, 'hindex': 28, 'hindex5y': 23, 'i10index': 51, 'i10index5y': 46}, {'name': 'Ting-Chao Chou', 'interests': ['Pharmacodynamics', 'Theoretical Biology', 'Chemotherapy', 'Drug Combination', 'Pharmacokinetics'], 'citedby': 43232, 'citedby5y': 10681, 'hindex': 77, 'hindex5y': 28, 'i10index': 295, 'i10index5y': 72}], 'repo_metadata': {'error': {}, 'empty': [], 'members': 2, 'member': ['keonlee9420', 'dependabot[bot]'], 'commits': 16, 'commit_name': ['keonlee9420', 'keonlee9420', 'keonlee9420', 'keonlee9420', 'keonlee9420', 'keonlee9420', 'keonlee9420', 'keonlee9420', 'keonlee9420', 'keonlee9420', 'keonlee9420', 'Keon Lee', 'Keon Lee', 'dependabot[bot]', 'dependabot[bot]', 'keonlee9420'], 'commit_date': ['2022-02-17T09:50:29Z', '2022-02-14T06:36:28Z', '2022-02-09T16:26:49Z', '2022-02-09T03:12:16Z', '2021-10-18T10:15:17Z', '2021-10-13T14:36:50Z', '2021-10-11T08:35:21Z', '2021-10-09T02:17:14Z', '2021-10-08T18:30:33Z', '2021-10-08T14:18:45Z', '2021-10-07T16:12:30Z', '2021-10-07T16:09:15Z', '2021-10-07T16:08:47Z', '2021-10-07T16:08:27Z', '2021-10-07T16:08:18Z', '2021-10-07T16:00:06Z'], 'description': 'PyTorch Implementation of PortaSpeech: Portable and High-Quality Generative Text-to-Speech', 'stars': 326, 'watchers': 20, 'forks': 35, 'issues': 11, 'default_branch': 'main', 'owner_type': 'User', 'organization_type': None, 'owner': 'keonlee9420', 'organization': None, 'has_wiki': True, 'prs': 2}, 'repo_members': [{'error': {}, 'empty': [], 'followers': 512, 'repos': ['keonlee9420/Comprehensive-E2E-TTS', 'keonlee9420/Comprehensive-Tacotron2', 'keonlee9420/Comprehensive-Transformer-TTS', 'keonlee9420/Cross-Speaker-Emotion-Transfer', 'keonlee9420/cs224n', 'keonlee9420/cs231n', 'keonlee9420/Daft-Exprt', 'keonlee9420/DailyTalk', 'keonlee9420/Deep-Learning-TTS-Template', 'keonlee9420/DiffGAN-TTS', 'keonlee9420/DiffSinger', 'keonlee9420/Expressive-FastSpeech2', 'keonlee9420/FastPitchFormant', 'keonlee9420/FastSpeech2', 'keonlee9420/FullSubNet', 'keonlee9420/Fully_Hierarchical_Fine_Grained_TTS', 'keonlee9420/Korean-FastSpeech2-Pytorch', 'keonlee9420/mellotron', 'keonlee9420/Parallel-Tacotron2', 'keonlee9420/pintos', 'keonlee9420/PortaSpeech', 'keonlee9420/Robust_Fine_Grained_Prosody_Control', 'keonlee9420/Soft-DTW-Loss', 'keonlee9420/speech-recognition-app', 'keonlee9420/SpeechSplit', 'keonlee9420/Stepwise_Monotonic_Multihead_Attention', 'keonlee9420/STYLER', 'keonlee9420/STYLER-Demo', 'keonlee9420/StyleSpeech', 'keonlee9420/tacotron2_MMI', 'keonlee9420/Transformer-tacotron2', 'keonlee9420/VAENAR-TTS', 'keonlee9420/WaveGrad2'], 'repo_count': 33, 'metadata': {'error': {'keonlee9420/Comprehensive-E2E-TTS': {}, 'keonlee9420/Comprehensive-Tacotron2': {}, 'keonlee9420/Comprehensive-Transformer-TTS': {}, 'keonlee9420/Cross-Speaker-Emotion-Transfer': {}, 'keonlee9420/cs224n': {}, 'keonlee9420/cs231n': {}, 'keonlee9420/Daft-Exprt': {}, 'keonlee9420/DailyTalk': {}, 'keonlee9420/Deep-Learning-TTS-Template': {}, 'keonlee9420/DiffGAN-TTS': {}, 'keonlee9420/DiffSinger': {}, 'keonlee9420/Expressive-FastSpeech2': {}, 'keonlee9420/FastPitchFormant': {}, 'keonlee9420/FastSpeech2': {}, 'keonlee9420/FullSubNet': {}, 'keonlee9420/Fully_Hierarchical_Fine_Grained_TTS': {}, 'keonlee9420/Korean-FastSpeech2-Pytorch': {}, 'keonlee9420/mellotron': {}, 'keonlee9420/Parallel-Tacotron2': {}, 'keonlee9420/pintos': {}, 'keonlee9420/PortaSpeech': {}, 'keonlee9420/Robust_Fine_Grained_Prosody_Control': {}, 'keonlee9420/Soft-DTW-Loss': {}, 'keonlee9420/speech-recognition-app': {}, 'keonlee9420/SpeechSplit': {}, 'keonlee9420/Stepwise_Monotonic_Multihead_Attention': {}, 'keonlee9420/STYLER': {}, 'keonlee9420/STYLER-Demo': {}, 'keonlee9420/StyleSpeech': {}, 'keonlee9420/tacotron2_MMI': {}, 'keonlee9420/Transformer-tacotron2': {}, 'keonlee9420/VAENAR-TTS': {}, 'keonlee9420/WaveGrad2': {}}, 'empty': {'keonlee9420/Comprehensive-E2E-TTS': [], 'keonlee9420/Comprehensive-Tacotron2': [], 'keonlee9420/Comprehensive-Transformer-TTS': [], 'keonlee9420/Cross-Speaker-Emotion-Transfer': ['prs'], 'keonlee9420/cs224n': ['prs'], 'keonlee9420/cs231n': [], 'keonlee9420/Daft-Exprt': [], 'keonlee9420/DailyTalk': [], 'keonlee9420/Deep-Learning-TTS-Template': ['prs'], 'keonlee9420/DiffGAN-TTS': ['prs'], 'keonlee9420/DiffSinger': ['prs'], 'keonlee9420/Expressive-FastSpeech2': [], 'keonlee9420/FastPitchFormant': ['prs'], 'keonlee9420/FastSpeech2': ['prs'], 'keonlee9420/FullSubNet': ['prs'], 'keonlee9420/Fully_Hierarchical_Fine_Grained_TTS': ['prs'], 'keonlee9420/Korean-FastSpeech2-Pytorch': ['prs'], 'keonlee9420/mellotron': ['prs'], 'keonlee9420/Parallel-Tacotron2': [], 'keonlee9420/pintos': ['prs'], 'keonlee9420/PortaSpeech': [], 'keonlee9420/Robust_Fine_Grained_Prosody_Control': [], 'keonlee9420/Soft-DTW-Loss': ['prs'], 'keonlee9420/speech-recognition-app': ['prs'], 'keonlee9420/SpeechSplit': ['prs'], 'keonlee9420/Stepwise_Monotonic_Multihead_Attention': ['prs'], 'keonlee9420/STYLER': [], 'keonlee9420/STYLER-Demo': ['prs'], 'keonlee9420/StyleSpeech': ['prs'], 'keonlee9420/tacotron2_MMI': ['prs'], 'keonlee9420/Transformer-tacotron2': ['prs'], 'keonlee9420/VAENAR-TTS': ['prs'], 'keonlee9420/WaveGrad2': []}, 'stars': 2881, 'forks': 438, 'issues': 120, 'watchers': 138, 'prs': 95}}, {'error': {}, 'empty': ['followers', 'repos'], 'followers': 0, 'repos': [], 'repo_count': 0, 'metadata': {'error': {}, 'empty': {}}}], 'repo_owner': {'name': 'keonlee9420', 'error': {}, 'empty': [], 'followers': 512, 'repos': ['keonlee9420/Comprehensive-E2E-TTS', 'keonlee9420/Comprehensive-Tacotron2', 'keonlee9420/Comprehensive-Transformer-TTS', 'keonlee9420/Cross-Speaker-Emotion-Transfer', 'keonlee9420/cs224n', 'keonlee9420/cs231n', 'keonlee9420/Daft-Exprt', 'keonlee9420/DailyTalk', 'keonlee9420/Deep-Learning-TTS-Template', 'keonlee9420/DiffGAN-TTS', 'keonlee9420/DiffSinger', 'keonlee9420/Expressive-FastSpeech2', 'keonlee9420/FastPitchFormant', 'keonlee9420/FastSpeech2', 'keonlee9420/FullSubNet', 'keonlee9420/Fully_Hierarchical_Fine_Grained_TTS', 'keonlee9420/Korean-FastSpeech2-Pytorch', 'keonlee9420/mellotron', 'keonlee9420/Parallel-Tacotron2', 'keonlee9420/pintos', 'keonlee9420/PortaSpeech', 'keonlee9420/Robust_Fine_Grained_Prosody_Control', 'keonlee9420/Soft-DTW-Loss', 'keonlee9420/speech-recognition-app', 'keonlee9420/SpeechSplit', 'keonlee9420/Stepwise_Monotonic_Multihead_Attention', 'keonlee9420/STYLER', 'keonlee9420/STYLER-Demo', 'keonlee9420/StyleSpeech', 'keonlee9420/tacotron2_MMI', 'keonlee9420/Transformer-tacotron2', 'keonlee9420/VAENAR-TTS', 'keonlee9420/WaveGrad2'], 'repo_count': 33, 'metadata': {'error': {'keonlee9420/Comprehensive-E2E-TTS': {}, 'keonlee9420/Comprehensive-Tacotron2': {}, 'keonlee9420/Comprehensive-Transformer-TTS': {}, 'keonlee9420/Cross-Speaker-Emotion-Transfer': {}, 'keonlee9420/cs224n': {}, 'keonlee9420/cs231n': {}, 'keonlee9420/Daft-Exprt': {}, 'keonlee9420/DailyTalk': {}, 'keonlee9420/Deep-Learning-TTS-Template': {}, 'keonlee9420/DiffGAN-TTS': {}, 'keonlee9420/DiffSinger': {}, 'keonlee9420/Expressive-FastSpeech2': {}, 'keonlee9420/FastPitchFormant': {}, 'keonlee9420/FastSpeech2': {}, 'keonlee9420/FullSubNet': {}, 'keonlee9420/Fully_Hierarchical_Fine_Grained_TTS': {}, 'keonlee9420/Korean-FastSpeech2-Pytorch': {}, 'keonlee9420/mellotron': {}, 'keonlee9420/Parallel-Tacotron2': {}, 'keonlee9420/pintos': {}, 'keonlee9420/PortaSpeech': {}, 'keonlee9420/Robust_Fine_Grained_Prosody_Control': {}, 'keonlee9420/Soft-DTW-Loss': {}, 'keonlee9420/speech-recognition-app': {}, 'keonlee9420/SpeechSplit': {}, 'keonlee9420/Stepwise_Monotonic_Multihead_Attention': {}, 'keonlee9420/STYLER': {}, 'keonlee9420/STYLER-Demo': {}, 'keonlee9420/StyleSpeech': {}, 'keonlee9420/tacotron2_MMI': {}, 'keonlee9420/Transformer-tacotron2': {}, 'keonlee9420/VAENAR-TTS': {}, 'keonlee9420/WaveGrad2': {}}, 'empty': {'keonlee9420/Comprehensive-E2E-TTS': [], 'keonlee9420/Comprehensive-Tacotron2': [], 'keonlee9420/Comprehensive-Transformer-TTS': [], 'keonlee9420/Cross-Speaker-Emotion-Transfer': ['prs'], 'keonlee9420/cs224n': ['prs'], 'keonlee9420/cs231n': [], 'keonlee9420/Daft-Exprt': [], 'keonlee9420/DailyTalk': [], 'keonlee9420/Deep-Learning-TTS-Template': ['prs'], 'keonlee9420/DiffGAN-TTS': ['prs'], 'keonlee9420/DiffSinger': ['prs'], 'keonlee9420/Expressive-FastSpeech2': [], 'keonlee9420/FastPitchFormant': ['prs'], 'keonlee9420/FastSpeech2': ['prs'], 'keonlee9420/FullSubNet': ['prs'], 'keonlee9420/Fully_Hierarchical_Fine_Grained_TTS': ['prs'], 'keonlee9420/Korean-FastSpeech2-Pytorch': ['prs'], 'keonlee9420/mellotron': ['prs'], 'keonlee9420/Parallel-Tacotron2': [], 'keonlee9420/pintos': ['prs'], 'keonlee9420/PortaSpeech': [], 'keonlee9420/Robust_Fine_Grained_Prosody_Control': [], 'keonlee9420/Soft-DTW-Loss': ['prs'], 'keonlee9420/speech-recognition-app': ['prs'], 'keonlee9420/SpeechSplit': ['prs'], 'keonlee9420/Stepwise_Monotonic_Multihead_Attention': ['prs'], 'keonlee9420/STYLER': [], 'keonlee9420/STYLER-Demo': ['prs'], 'keonlee9420/StyleSpeech': ['prs'], 'keonlee9420/tacotron2_MMI': ['prs'], 'keonlee9420/Transformer-tacotron2': ['prs'], 'keonlee9420/VAENAR-TTS': ['prs'], 'keonlee9420/WaveGrad2': []}, 'stars': 2881, 'forks': 438, 'issues': 120, 'watchers': 138, 'prs': 95}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def parse_repository_data(data):\n",
    "    # Extracting the paper URL as the key of the input data\n",
    "    paper_url = list(data.keys())[0]\n",
    "    \n",
    "    # Building the path to the paper's metadata using the paper URL\n",
    "    paper_metadata_path = paper_url.replace('https://arxiv.org/abs/', '../depot/papers/') + '/metadata.json'\n",
    "    authors_paths = data[paper_url]['authors']\n",
    "    \n",
    "    # Extracting paths for repository's metadata, members, and owner\n",
    "    repo_metadata_path = data[paper_url]['metadata']\n",
    "    repo_members_paths = data[paper_url]['members']\n",
    "    repo_owner_path = data[paper_url]['owner']\n",
    "    \n",
    "    # Loading JSON data from the paths\n",
    "    with open(paper_metadata_path, 'r') as file:\n",
    "        paper_metadata = json.load(file)\n",
    "      \n",
    "    authors = [] \n",
    "    for author_path in authors_paths:\n",
    "        with open(author_path, 'r') as file:\n",
    "            author_data = json.load(file)\n",
    "            authors.append({x: author_data[x] for x in author_data if x not in ['publications', 'cites_per_year', 'affiliation']})\n",
    "    \n",
    "    repo_metadata = None\n",
    "    if repo_metadata_path:\n",
    "        with open(repo_metadata_path, 'r') as file:\n",
    "            repo_metadata = json.load(file)\n",
    "    \n",
    "    repo_members = []\n",
    "    for member_path in repo_members_paths:\n",
    "        with open(member_path, 'r') as file:\n",
    "            member_data = json.load(file)\n",
    "            repo_members.append(member_data)\n",
    "    \n",
    "    with open(repo_owner_path, 'r') as file:\n",
    "        repo_owner = json.load(file)\n",
    "    \n",
    "    return {\n",
    "        'paper_metadata': paper_metadata,\n",
    "        'author_data': authors,\n",
    "        'repo_metadata': repo_metadata,\n",
    "        'repo_members': repo_members,\n",
    "        'repo_owner': repo_owner\n",
    "    }\n",
    "\n",
    "# Assuming 'input_data' is your provided data structure\n",
    "input_data = {\n",
    "    \"https://arxiv.org/abs/2109.15166\": {\n",
    "        \"paper\": \"../depot/papers/2109.15166/PortaSpeech: Portable and High-Quality Generative Text-to-Speech.pdf\",\n",
    "        \"metadata\": \"../depot/repository/keonlee9420_PortaSpeech/metadata.json\",\n",
    "        \"authors\": [\"../depot/papers/authors/Yi Ren.json\", \"../depot/papers/authors/Jinglin Liu.json\", \"../depot/papers/authors/Zhou Zhao.json\"],\n",
    "        \"repo\": \"PortaSpeech\",\n",
    "        \"branch\": \"main\",\n",
    "        \"tree\": \"../depot/repository/keonlee9420_PortaSpeech/tree.json\",\n",
    "        \"commit_history\": \"../depot/repository/keonlee9420_PortaSpeech/commit_history.json\",\n",
    "        \"tree_formatted\": \"../depot/repository/keonlee9420_PortaSpeech/tree_formatted.txt\",\n",
    "        \"owner\": \"../depot/repository/owner/keonlee9420.json\",\n",
    "        \"organization\": None,\n",
    "        \"members\": [\"../depot/repository/members/keonlee9420.json\", \"../depot/repository/members/dependabot[bot].json\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "parsed_data = parse_repository_data(input_data)\n",
    "print(parsed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"paper_metadata\": {\n",
      "    \"title\": \"PortaSpeech: Portable and High-Quality Generative Text-to-Speech\",\n",
      "    \"authors\": [\n",
      "      \"Yi Ren\",\n",
      "      \"Jinglin Liu\",\n",
      "      \"Zhou Zhao\"\n",
      "    ],\n",
      "    \"summary\": \"Non-autoregressive text-to-speech (NAR-TTS) models such as FastSpeech 2 and\\nGlow-TTS can synthesize high-quality speech from the given text in parallel.\\nAfter analyzing two kinds of generative NAR-TTS models (VAE and normalizing\\nflow), we find that: VAE is good at capturing the long-range semantics features\\n(e.g., prosody) even with small model size but suffers from blurry and\\nunnatural results; and normalizing flow is good at reconstructing the frequency\\nbin-wise details but performs poorly when the number of model parameters is\\nlimited. Inspired by these observations, to generate diverse speech with\\nnatural details and rich prosody using a lightweight architecture, we propose\\nPortaSpeech, a portable and high-quality generative text-to-speech model.\\nSpecifically, 1) to model both the prosody and mel-spectrogram details\\naccurately, we adopt a lightweight VAE with an enhanced prior followed by a\\nflow-based post-net with strong conditional inputs as the main architecture. 2)\\nTo further compress the model size and memory footprint, we introduce the\\ngrouped parameter sharing mechanism to the affine coupling layers in the\\npost-net. 3) To improve the expressiveness of synthesized speech and reduce the\\ndependency on accurate fine-grained alignment between text and speech, we\\npropose a linguistic encoder with mixture alignment combining hard inter-word\\nalignment and soft intra-word alignment, which explicitly extracts word-level\\nsemantic information. Experimental results show that PortaSpeech outperforms\\nother TTS models in both voice quality and prosody modeling in terms of\\nsubjective and objective evaluation metrics, and shows only a slight\\nperformance degradation when reducing the model parameters to 6.7M (about 4x\\nmodel size and 3x runtime memory compression ratio compared with FastSpeech 2).\\nOur extensive ablation studies demonstrate that each design in PortaSpeech is\\neffective.\",\n",
      "    \"published\": \"09/30/2021\",\n",
      "    \"pdf_url\": \"http://arxiv.org/pdf/2109.15166v5\"\n",
      "  },\n",
      "  \"author_data\": [\n",
      "    {\n",
      "      \"name\": \"Yi Ren\",\n",
      "      \"interests\": [\n",
      "        \"cell biology\"\n",
      "      ],\n",
      "      \"citedby\": 8743,\n",
      "      \"citedby5y\": 2908,\n",
      "      \"hindex\": 43,\n",
      "      \"hindex5y\": 29,\n",
      "      \"i10index\": 72,\n",
      "      \"i10index5y\": 54\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Jinglin Liu\",\n",
      "      \"interests\": [\n",
      "        \"plasma catalysis\",\n",
      "        \"gliding arc\",\n",
      "        \"methane pyrolysis\",\n",
      "        \"air pollution control\",\n",
      "        \"plasma chemistry\"\n",
      "      ],\n",
      "      \"citedby\": 1986,\n",
      "      \"citedby5y\": 1403,\n",
      "      \"hindex\": 28,\n",
      "      \"hindex5y\": 23,\n",
      "      \"i10index\": 51,\n",
      "      \"i10index5y\": 46\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Ting-Chao Chou\",\n",
      "      \"interests\": [\n",
      "        \"Pharmacodynamics\",\n",
      "        \"Theoretical Biology\",\n",
      "        \"Chemotherapy\",\n",
      "        \"Drug Combination\",\n",
      "        \"Pharmacokinetics\"\n",
      "      ],\n",
      "      \"citedby\": 43232,\n",
      "      \"citedby5y\": 10681,\n",
      "      \"hindex\": 77,\n",
      "      \"hindex5y\": 28,\n",
      "      \"i10index\": 295,\n",
      "      \"i10index5y\": 72\n",
      "    }\n",
      "  ],\n",
      "  \"repo_metadata\": {\n",
      "    \"error\": {},\n",
      "    \"empty\": [],\n",
      "    \"members\": 2,\n",
      "    \"member\": [\n",
      "      \"keonlee9420\",\n",
      "      \"dependabot[bot]\"\n",
      "    ],\n",
      "    \"commits\": 16,\n",
      "    \"commit_name\": [\n",
      "      \"keonlee9420\",\n",
      "      \"keonlee9420\",\n",
      "      \"keonlee9420\",\n",
      "      \"keonlee9420\",\n",
      "      \"keonlee9420\",\n",
      "      \"keonlee9420\",\n",
      "      \"keonlee9420\",\n",
      "      \"keonlee9420\",\n",
      "      \"keonlee9420\",\n",
      "      \"keonlee9420\",\n",
      "      \"keonlee9420\",\n",
      "      \"Keon Lee\",\n",
      "      \"Keon Lee\",\n",
      "      \"dependabot[bot]\",\n",
      "      \"dependabot[bot]\",\n",
      "      \"keonlee9420\"\n",
      "    ],\n",
      "    \"commit_date\": [\n",
      "      \"2022-02-17T09:50:29Z\",\n",
      "      \"2022-02-14T06:36:28Z\",\n",
      "      \"2022-02-09T16:26:49Z\",\n",
      "      \"2022-02-09T03:12:16Z\",\n",
      "      \"2021-10-18T10:15:17Z\",\n",
      "      \"2021-10-13T14:36:50Z\",\n",
      "      \"2021-10-11T08:35:21Z\",\n",
      "      \"2021-10-09T02:17:14Z\",\n",
      "      \"2021-10-08T18:30:33Z\",\n",
      "      \"2021-10-08T14:18:45Z\",\n",
      "      \"2021-10-07T16:12:30Z\",\n",
      "      \"2021-10-07T16:09:15Z\",\n",
      "      \"2021-10-07T16:08:47Z\",\n",
      "      \"2021-10-07T16:08:27Z\",\n",
      "      \"2021-10-07T16:08:18Z\",\n",
      "      \"2021-10-07T16:00:06Z\"\n",
      "    ],\n",
      "    \"description\": \"PyTorch Implementation of PortaSpeech: Portable and High-Quality Generative Text-to-Speech\",\n",
      "    \"stars\": 326,\n",
      "    \"watchers\": 20,\n",
      "    \"forks\": 35,\n",
      "    \"issues\": 11,\n",
      "    \"default_branch\": \"main\",\n",
      "    \"owner_type\": \"User\",\n",
      "    \"organization_type\": null,\n",
      "    \"owner\": \"keonlee9420\",\n",
      "    \"organization\": null,\n",
      "    \"has_wiki\": true,\n",
      "    \"prs\": 2\n",
      "  },\n",
      "  \"repo_members\": [\n",
      "    {\n",
      "      \"error\": {},\n",
      "      \"empty\": [],\n",
      "      \"followers\": 512,\n",
      "      \"repos\": [\n",
      "        \"keonlee9420/Comprehensive-E2E-TTS\",\n",
      "        \"keonlee9420/Comprehensive-Tacotron2\",\n",
      "        \"keonlee9420/Comprehensive-Transformer-TTS\",\n",
      "        \"keonlee9420/Cross-Speaker-Emotion-Transfer\",\n",
      "        \"keonlee9420/cs224n\",\n",
      "        \"keonlee9420/cs231n\",\n",
      "        \"keonlee9420/Daft-Exprt\",\n",
      "        \"keonlee9420/DailyTalk\",\n",
      "        \"keonlee9420/Deep-Learning-TTS-Template\",\n",
      "        \"keonlee9420/DiffGAN-TTS\",\n",
      "        \"keonlee9420/DiffSinger\",\n",
      "        \"keonlee9420/Expressive-FastSpeech2\",\n",
      "        \"keonlee9420/FastPitchFormant\",\n",
      "        \"keonlee9420/FastSpeech2\",\n",
      "        \"keonlee9420/FullSubNet\",\n",
      "        \"keonlee9420/Fully_Hierarchical_Fine_Grained_TTS\",\n",
      "        \"keonlee9420/Korean-FastSpeech2-Pytorch\",\n",
      "        \"keonlee9420/mellotron\",\n",
      "        \"keonlee9420/Parallel-Tacotron2\",\n",
      "        \"keonlee9420/pintos\",\n",
      "        \"keonlee9420/PortaSpeech\",\n",
      "        \"keonlee9420/Robust_Fine_Grained_Prosody_Control\",\n",
      "        \"keonlee9420/Soft-DTW-Loss\",\n",
      "        \"keonlee9420/speech-recognition-app\",\n",
      "        \"keonlee9420/SpeechSplit\",\n",
      "        \"keonlee9420/Stepwise_Monotonic_Multihead_Attention\",\n",
      "        \"keonlee9420/STYLER\",\n",
      "        \"keonlee9420/STYLER-Demo\",\n",
      "        \"keonlee9420/StyleSpeech\",\n",
      "        \"keonlee9420/tacotron2_MMI\",\n",
      "        \"keonlee9420/Transformer-tacotron2\",\n",
      "        \"keonlee9420/VAENAR-TTS\",\n",
      "        \"keonlee9420/WaveGrad2\"\n",
      "      ],\n",
      "      \"repo_count\": 33,\n",
      "      \"metadata\": {\n",
      "        \"error\": {\n",
      "          \"keonlee9420/Comprehensive-E2E-TTS\": {},\n",
      "          \"keonlee9420/Comprehensive-Tacotron2\": {},\n",
      "          \"keonlee9420/Comprehensive-Transformer-TTS\": {},\n",
      "          \"keonlee9420/Cross-Speaker-Emotion-Transfer\": {},\n",
      "          \"keonlee9420/cs224n\": {},\n",
      "          \"keonlee9420/cs231n\": {},\n",
      "          \"keonlee9420/Daft-Exprt\": {},\n",
      "          \"keonlee9420/DailyTalk\": {},\n",
      "          \"keonlee9420/Deep-Learning-TTS-Template\": {},\n",
      "          \"keonlee9420/DiffGAN-TTS\": {},\n",
      "          \"keonlee9420/DiffSinger\": {},\n",
      "          \"keonlee9420/Expressive-FastSpeech2\": {},\n",
      "          \"keonlee9420/FastPitchFormant\": {},\n",
      "          \"keonlee9420/FastSpeech2\": {},\n",
      "          \"keonlee9420/FullSubNet\": {},\n",
      "          \"keonlee9420/Fully_Hierarchical_Fine_Grained_TTS\": {},\n",
      "          \"keonlee9420/Korean-FastSpeech2-Pytorch\": {},\n",
      "          \"keonlee9420/mellotron\": {},\n",
      "          \"keonlee9420/Parallel-Tacotron2\": {},\n",
      "          \"keonlee9420/pintos\": {},\n",
      "          \"keonlee9420/PortaSpeech\": {},\n",
      "          \"keonlee9420/Robust_Fine_Grained_Prosody_Control\": {},\n",
      "          \"keonlee9420/Soft-DTW-Loss\": {},\n",
      "          \"keonlee9420/speech-recognition-app\": {},\n",
      "          \"keonlee9420/SpeechSplit\": {},\n",
      "          \"keonlee9420/Stepwise_Monotonic_Multihead_Attention\": {},\n",
      "          \"keonlee9420/STYLER\": {},\n",
      "          \"keonlee9420/STYLER-Demo\": {},\n",
      "          \"keonlee9420/StyleSpeech\": {},\n",
      "          \"keonlee9420/tacotron2_MMI\": {},\n",
      "          \"keonlee9420/Transformer-tacotron2\": {},\n",
      "          \"keonlee9420/VAENAR-TTS\": {},\n",
      "          \"keonlee9420/WaveGrad2\": {}\n",
      "        },\n",
      "        \"empty\": {\n",
      "          \"keonlee9420/Comprehensive-E2E-TTS\": [],\n",
      "          \"keonlee9420/Comprehensive-Tacotron2\": [],\n",
      "          \"keonlee9420/Comprehensive-Transformer-TTS\": [],\n",
      "          \"keonlee9420/Cross-Speaker-Emotion-Transfer\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/cs224n\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/cs231n\": [],\n",
      "          \"keonlee9420/Daft-Exprt\": [],\n",
      "          \"keonlee9420/DailyTalk\": [],\n",
      "          \"keonlee9420/Deep-Learning-TTS-Template\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/DiffGAN-TTS\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/DiffSinger\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/Expressive-FastSpeech2\": [],\n",
      "          \"keonlee9420/FastPitchFormant\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/FastSpeech2\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/FullSubNet\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/Fully_Hierarchical_Fine_Grained_TTS\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/Korean-FastSpeech2-Pytorch\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/mellotron\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/Parallel-Tacotron2\": [],\n",
      "          \"keonlee9420/pintos\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/PortaSpeech\": [],\n",
      "          \"keonlee9420/Robust_Fine_Grained_Prosody_Control\": [],\n",
      "          \"keonlee9420/Soft-DTW-Loss\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/speech-recognition-app\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/SpeechSplit\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/Stepwise_Monotonic_Multihead_Attention\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/STYLER\": [],\n",
      "          \"keonlee9420/STYLER-Demo\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/StyleSpeech\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/tacotron2_MMI\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/Transformer-tacotron2\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/VAENAR-TTS\": [\n",
      "            \"prs\"\n",
      "          ],\n",
      "          \"keonlee9420/WaveGrad2\": []\n",
      "        },\n",
      "        \"stars\": 2881,\n",
      "        \"forks\": 438,\n",
      "        \"issues\": 120,\n",
      "        \"watchers\": 138,\n",
      "        \"prs\": 95\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"error\": {},\n",
      "      \"empty\": [\n",
      "        \"followers\",\n",
      "        \"repos\"\n",
      "      ],\n",
      "      \"followers\": 0,\n",
      "      \"repos\": [],\n",
      "      \"repo_count\": 0,\n",
      "      \"metadata\": {\n",
      "        \"error\": {},\n",
      "        \"empty\": {}\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"repo_owner\": {\n",
      "    \"name\": \"keonlee9420\",\n",
      "    \"error\": {},\n",
      "    \"empty\": [],\n",
      "    \"followers\": 512,\n",
      "    \"repos\": [\n",
      "      \"keonlee9420/Comprehensive-E2E-TTS\",\n",
      "      \"keonlee9420/Comprehensive-Tacotron2\",\n",
      "      \"keonlee9420/Comprehensive-Transformer-TTS\",\n",
      "      \"keonlee9420/Cross-Speaker-Emotion-Transfer\",\n",
      "      \"keonlee9420/cs224n\",\n",
      "      \"keonlee9420/cs231n\",\n",
      "      \"keonlee9420/Daft-Exprt\",\n",
      "      \"keonlee9420/DailyTalk\",\n",
      "      \"keonlee9420/Deep-Learning-TTS-Template\",\n",
      "      \"keonlee9420/DiffGAN-TTS\",\n",
      "      \"keonlee9420/DiffSinger\",\n",
      "      \"keonlee9420/Expressive-FastSpeech2\",\n",
      "      \"keonlee9420/FastPitchFormant\",\n",
      "      \"keonlee9420/FastSpeech2\",\n",
      "      \"keonlee9420/FullSubNet\",\n",
      "      \"keonlee9420/Fully_Hierarchical_Fine_Grained_TTS\",\n",
      "      \"keonlee9420/Korean-FastSpeech2-Pytorch\",\n",
      "      \"keonlee9420/mellotron\",\n",
      "      \"keonlee9420/Parallel-Tacotron2\",\n",
      "      \"keonlee9420/pintos\",\n",
      "      \"keonlee9420/PortaSpeech\",\n",
      "      \"keonlee9420/Robust_Fine_Grained_Prosody_Control\",\n",
      "      \"keonlee9420/Soft-DTW-Loss\",\n",
      "      \"keonlee9420/speech-recognition-app\",\n",
      "      \"keonlee9420/SpeechSplit\",\n",
      "      \"keonlee9420/Stepwise_Monotonic_Multihead_Attention\",\n",
      "      \"keonlee9420/STYLER\",\n",
      "      \"keonlee9420/STYLER-Demo\",\n",
      "      \"keonlee9420/StyleSpeech\",\n",
      "      \"keonlee9420/tacotron2_MMI\",\n",
      "      \"keonlee9420/Transformer-tacotron2\",\n",
      "      \"keonlee9420/VAENAR-TTS\",\n",
      "      \"keonlee9420/WaveGrad2\"\n",
      "    ],\n",
      "    \"repo_count\": 33,\n",
      "    \"metadata\": {\n",
      "      \"error\": {\n",
      "        \"keonlee9420/Comprehensive-E2E-TTS\": {},\n",
      "        \"keonlee9420/Comprehensive-Tacotron2\": {},\n",
      "        \"keonlee9420/Comprehensive-Transformer-TTS\": {},\n",
      "        \"keonlee9420/Cross-Speaker-Emotion-Transfer\": {},\n",
      "        \"keonlee9420/cs224n\": {},\n",
      "        \"keonlee9420/cs231n\": {},\n",
      "        \"keonlee9420/Daft-Exprt\": {},\n",
      "        \"keonlee9420/DailyTalk\": {},\n",
      "        \"keonlee9420/Deep-Learning-TTS-Template\": {},\n",
      "        \"keonlee9420/DiffGAN-TTS\": {},\n",
      "        \"keonlee9420/DiffSinger\": {},\n",
      "        \"keonlee9420/Expressive-FastSpeech2\": {},\n",
      "        \"keonlee9420/FastPitchFormant\": {},\n",
      "        \"keonlee9420/FastSpeech2\": {},\n",
      "        \"keonlee9420/FullSubNet\": {},\n",
      "        \"keonlee9420/Fully_Hierarchical_Fine_Grained_TTS\": {},\n",
      "        \"keonlee9420/Korean-FastSpeech2-Pytorch\": {},\n",
      "        \"keonlee9420/mellotron\": {},\n",
      "        \"keonlee9420/Parallel-Tacotron2\": {},\n",
      "        \"keonlee9420/pintos\": {},\n",
      "        \"keonlee9420/PortaSpeech\": {},\n",
      "        \"keonlee9420/Robust_Fine_Grained_Prosody_Control\": {},\n",
      "        \"keonlee9420/Soft-DTW-Loss\": {},\n",
      "        \"keonlee9420/speech-recognition-app\": {},\n",
      "        \"keonlee9420/SpeechSplit\": {},\n",
      "        \"keonlee9420/Stepwise_Monotonic_Multihead_Attention\": {},\n",
      "        \"keonlee9420/STYLER\": {},\n",
      "        \"keonlee9420/STYLER-Demo\": {},\n",
      "        \"keonlee9420/StyleSpeech\": {},\n",
      "        \"keonlee9420/tacotron2_MMI\": {},\n",
      "        \"keonlee9420/Transformer-tacotron2\": {},\n",
      "        \"keonlee9420/VAENAR-TTS\": {},\n",
      "        \"keonlee9420/WaveGrad2\": {}\n",
      "      },\n",
      "      \"empty\": {\n",
      "        \"keonlee9420/Comprehensive-E2E-TTS\": [],\n",
      "        \"keonlee9420/Comprehensive-Tacotron2\": [],\n",
      "        \"keonlee9420/Comprehensive-Transformer-TTS\": [],\n",
      "        \"keonlee9420/Cross-Speaker-Emotion-Transfer\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/cs224n\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/cs231n\": [],\n",
      "        \"keonlee9420/Daft-Exprt\": [],\n",
      "        \"keonlee9420/DailyTalk\": [],\n",
      "        \"keonlee9420/Deep-Learning-TTS-Template\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/DiffGAN-TTS\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/DiffSinger\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/Expressive-FastSpeech2\": [],\n",
      "        \"keonlee9420/FastPitchFormant\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/FastSpeech2\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/FullSubNet\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/Fully_Hierarchical_Fine_Grained_TTS\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/Korean-FastSpeech2-Pytorch\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/mellotron\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/Parallel-Tacotron2\": [],\n",
      "        \"keonlee9420/pintos\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/PortaSpeech\": [],\n",
      "        \"keonlee9420/Robust_Fine_Grained_Prosody_Control\": [],\n",
      "        \"keonlee9420/Soft-DTW-Loss\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/speech-recognition-app\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/SpeechSplit\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/Stepwise_Monotonic_Multihead_Attention\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/STYLER\": [],\n",
      "        \"keonlee9420/STYLER-Demo\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/StyleSpeech\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/tacotron2_MMI\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/Transformer-tacotron2\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/VAENAR-TTS\": [\n",
      "          \"prs\"\n",
      "        ],\n",
      "        \"keonlee9420/WaveGrad2\": []\n",
      "      },\n",
      "      \"stars\": 2881,\n",
      "      \"forks\": 438,\n",
      "      \"issues\": 120,\n",
      "      \"watchers\": 138,\n",
      "      \"prs\": 95\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(parsed_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compressing metadata\n",
    "paper_metadata = { \n",
    "  authors: parsed_data['paper_metadata']['authors'], \n",
    "  author_data: {author:  for author in parsed_data['author_data']},\n",
    "  \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
